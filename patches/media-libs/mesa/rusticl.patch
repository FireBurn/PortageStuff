diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index 6ad93886bbd..258cf88cdc8 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -683,6 +683,23 @@ static bool visit_alu(struct ac_nir_context *ctx, const nir_alu_instr *instr)
    case nir_op_fmul:
       src[0] = ac_to_float(&ctx->ac, src[0]);
       src[1] = ac_to_float(&ctx->ac, src[1]);
+      if (nir_src_is_const(instr->src[0].src) && nir_src_as_float(instr->src[0].src) == 1.0) {
+         if (ac_get_type_size(def_type) == 8) {
+            result = ac_build_intrinsic(&ctx->ac, "llvm.canonicalize.f64", ctx->ac.f64, &src[1], 1, AC_FUNC_ATTR_READNONE);
+            break;
+         } else if (ac_get_type_size(def_type) == 4) {
+            result = ac_build_intrinsic(&ctx->ac, "llvm.canonicalize.f32", ctx->ac.f32, &src[1], 1, AC_FUNC_ATTR_READNONE);
+            break;
+         }
+      } else if (nir_src_is_const(instr->src[1].src) && nir_src_as_float(instr->src[1].src) == 1.0) {
+         if (ac_get_type_size(def_type) == 8) {
+            result = ac_build_intrinsic(&ctx->ac, "llvm.canonicalize.f64", ctx->ac.f64, &src[0], 1, AC_FUNC_ATTR_READNONE);
+            break;
+         } else if (ac_get_type_size(def_type) == 4) {
+            result = ac_build_intrinsic(&ctx->ac, "llvm.canonicalize.f32", ctx->ac.f32, &src[0], 1, AC_FUNC_ATTR_READNONE);
+            break;
+         }
+      }
       result = LLVMBuildFMul(ctx->ac.builder, src[0], src[1], "");
       break;
    case nir_op_fmulz:
@@ -997,7 +1014,6 @@ static bool visit_alu(struct ac_nir_context *ctx, const nir_alu_instr *instr)
       result = LLVMBuildUIToFP(ctx->ac.builder, src[0], ac_to_float_type(&ctx->ac, def_type), "");
       break;
    case nir_op_f2f16_rtz:
-   case nir_op_f2f16:
    case nir_op_f2fmp:
       src[0] = ac_to_float(&ctx->ac, src[0]);
 
@@ -1036,6 +1052,7 @@ static bool visit_alu(struct ac_nir_context *ctx, const nir_alu_instr *instr)
                LLVMBuildFPTrunc(ctx->ac.builder, src[0], ac_to_float_type(&ctx->ac, def_type), "");
       }
       break;
+   case nir_op_f2f16:
    case nir_op_f2f16_rtne:
    case nir_op_f2f32:
    case nir_op_f2f64:
@@ -2310,31 +2327,32 @@ static LLVMValueRef visit_load_ubo_buffer(struct ac_nir_context *ctx, nir_intrin
    if (ctx->abi->load_ubo)
       rsrc = ctx->abi->load_ubo(ctx->abi, rsrc);
 
-   /* Convert to a scalar 32-bit load. */
    if (instr->dest.ssa.bit_size == 64)
       num_components *= 2;
-   else if (instr->dest.ssa.bit_size == 16)
-      num_components = DIV_ROUND_UP(num_components, 2);
-   else if (instr->dest.ssa.bit_size == 8)
-      num_components = DIV_ROUND_UP(num_components, 4);
 
-   ret =
-      ac_build_buffer_load(&ctx->ac, rsrc, num_components, NULL, offset, NULL,
-                           ctx->ac.f32, 0, true, true);
+   if (instr->dest.ssa.bit_size == 16 || instr->dest.ssa.bit_size == 8) {
+      unsigned load_bytes = instr->dest.ssa.bit_size / 8;
+      LLVMValueRef *const results = alloca(num_components * sizeof(LLVMValueRef));
+      for (unsigned i = 0; i < num_components; ++i) {
+         LLVMValueRef immoffset = LLVMConstInt(ctx->ac.i32, load_bytes * i, 0);
 
-   /* Convert to the original type. */
-   if (instr->dest.ssa.bit_size == 64) {
-      ret = LLVMBuildBitCast(ctx->ac.builder, ret,
-                             LLVMVectorType(ctx->ac.i64, num_components / 2), "");
-   } else if (instr->dest.ssa.bit_size == 16) {
-      ret = LLVMBuildBitCast(ctx->ac.builder, ret,
-                             LLVMVectorType(ctx->ac.i16, num_components * 2), "");
-   } else if (instr->dest.ssa.bit_size == 8) {
-      ret = LLVMBuildBitCast(ctx->ac.builder, ret,
-                             LLVMVectorType(ctx->ac.i8, num_components * 4), "");
+         if (load_bytes == 1) {
+            results[i] =
+               ac_build_buffer_load_byte(&ctx->ac, rsrc, offset, immoffset, 0);
+         } else {
+            assert(load_bytes == 2);
+            results[i] =
+               ac_build_buffer_load_short(&ctx->ac, rsrc, offset, immoffset, 0);
+         }
+      }
+      ret = ac_build_gather_values(&ctx->ac, results, num_components);
+   } else {
+      ret =
+         ac_build_buffer_load(&ctx->ac, rsrc, num_components, NULL, offset, NULL, 0, 0, true, true);
+
+      ret = ac_trim_vector(&ctx->ac, ret, num_components);
    }
 
-   ret = ac_trim_vector(&ctx->ac, ret, instr->num_components);
    ret = LLVMBuildBitCast(ctx->ac.builder, ret, get_def_type(ctx, &instr->dest.ssa), "");
 
    return exit_waterfall(ctx, &wctx, ret);
diff --git a/src/amd/vulkan/radv_pipeline_rt.c b/src/amd/vulkan/radv_pipeline_rt.c
index a5ebf0d79e1..3e39b8d8fd3 100644
--- a/src/amd/vulkan/radv_pipeline_rt.c
+++ b/src/amd/vulkan/radv_pipeline_rt.c
@@ -856,7 +856,7 @@ parse_rt_stage(struct radv_device *device, const VkPipelineShaderStageCreateInfo
 
    NIR_PASS(_, shader, lower_rt_derefs);
 
-   NIR_PASS(_, shader, nir_lower_explicit_io, nir_var_function_temp,
+   NIR_PASS(_, shader, nir_lower_explicit_io, nir_var_function_temp, false,
             nir_address_format_32bit_offset);
 
    return shader;
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 94275b769e6..0504112c4fb 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -1055,9 +1055,9 @@ radv_shader_spirv_to_nir(struct radv_device *device, const struct radv_pipeline_
    };
    NIR_PASS(_, nir, nir_opt_access, &opt_access_options);
 
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_push_const, nir_address_format_32bit_offset);
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_push_const, false, nir_address_format_32bit_offset);
 
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ubo | nir_var_mem_ssbo,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ubo | nir_var_mem_ssbo, false,
             nir_address_format_vec2_index_32bit_offset);
 
    NIR_PASS(_, nir, lower_intrinsics, key);
@@ -1075,7 +1075,7 @@ radv_shader_spirv_to_nir(struct radv_device *device, const struct radv_pipeline_
       if (!nir->info.shared_memory_explicit_layout) {
          NIR_PASS(_, nir, nir_lower_vars_to_explicit_types, var_modes, shared_var_info);
       }
-      NIR_PASS(_, nir, nir_lower_explicit_io, var_modes, nir_address_format_32bit_offset);
+      NIR_PASS(_, nir, nir_lower_explicit_io, var_modes, false, nir_address_format_32bit_offset);
 
       if (nir->info.zero_initialize_shared_memory && nir->info.shared_size > 0) {
          const unsigned chunk_size = 16; /* max single store size */
@@ -1084,7 +1084,7 @@ radv_shader_spirv_to_nir(struct radv_device *device, const struct radv_pipeline_
       }
    }
 
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_global | nir_var_mem_constant,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_global | nir_var_mem_constant, false,
             nir_address_format_64bit_global);
 
    /* Lower large variables that are always constant with load_constant
diff --git a/src/compiler/glsl/gl_nir_lower_buffers.c b/src/compiler/glsl/gl_nir_lower_buffers.c
index 12a2aebb858..0dfb4085b59 100644
--- a/src/compiler/glsl/gl_nir_lower_buffers.c
+++ b/src/compiler/glsl/gl_nir_lower_buffers.c
@@ -353,7 +353,7 @@ gl_nir_lower_buffers(nir_shader *shader,
     */
    if (progress) {
       nir_validate_shader(shader, "Lowering buffer interface derefs");
-      nir_lower_explicit_io(shader, nir_var_mem_ubo | nir_var_mem_ssbo,
+      nir_lower_explicit_io(shader, nir_var_mem_ubo | nir_var_mem_ssbo, false,
                             nir_address_format_32bit_index_offset);
    }
 
diff --git a/src/compiler/glsl_types.cpp b/src/compiler/glsl_types.cpp
index 63190eec9dd..ceb6a0f1322 100644
--- a/src/compiler/glsl_types.cpp
+++ b/src/compiler/glsl_types.cpp
@@ -974,9 +974,9 @@ glsl_type::get_texture_instance(enum glsl_sampler_dim dim,
       case GLSL_SAMPLER_DIM_MS:
          return (array ? texture2DMSArray_type : texture2DMS_type);
       case GLSL_SAMPLER_DIM_SUBPASS:
-         return subpassInput_type;
+         return texture2D_type;
       case GLSL_SAMPLER_DIM_SUBPASS_MS:
-         return subpassInputMS_type;
+         return texture2DMS_type;
       case GLSL_SAMPLER_DIM_EXTERNAL:
          if (array)
             return error_type;
@@ -1006,9 +1006,9 @@ glsl_type::get_texture_instance(enum glsl_sampler_dim dim,
       case GLSL_SAMPLER_DIM_MS:
          return (array ? itexture2DMSArray_type : itexture2DMS_type);
       case GLSL_SAMPLER_DIM_SUBPASS:
-         return isubpassInput_type;
+         return itexture2D_type;
       case GLSL_SAMPLER_DIM_SUBPASS_MS:
-         return isubpassInputMS_type;
+         return itexture2DMS_type;
       case GLSL_SAMPLER_DIM_EXTERNAL:
          return error_type;
       }
@@ -1035,9 +1035,9 @@ glsl_type::get_texture_instance(enum glsl_sampler_dim dim,
       case GLSL_SAMPLER_DIM_MS:
          return (array ? utexture2DMSArray_type : utexture2DMS_type);
       case GLSL_SAMPLER_DIM_SUBPASS:
-         return usubpassInput_type;
+         return utexture2D_type;
       case GLSL_SAMPLER_DIM_SUBPASS_MS:
-         return usubpassInputMS_type;
+         return utexture2DMS_type;
       case GLSL_SAMPLER_DIM_EXTERNAL:
          return error_type;
       }
@@ -1050,7 +1050,7 @@ glsl_type::get_texture_instance(enum glsl_sampler_dim dim,
       case GLSL_SAMPLER_DIM_3D:
          return (array ? error_type : vtexture3D_type);
       case GLSL_SAMPLER_DIM_BUF:
-         return (array ? error_type : vbuffer_type);
+         return (array ? error_type : vtextureBuffer_type);
       default:
          return error_type;
       }
diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index afc53ac0557..2c4506bda92 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -4832,6 +4832,7 @@ void nir_lower_explicit_io_instr(struct nir_builder *b,
 
 bool nir_lower_explicit_io(nir_shader *shader,
                            nir_variable_mode modes,
+                           bool skip_samplers,
                            nir_address_format);
 
 typedef struct nir_lower_shader_calls_options {
@@ -5493,7 +5494,7 @@ bool nir_lower_ssa_defs_to_regs_block(nir_block *block);
 bool nir_rematerialize_derefs_in_use_blocks_impl(nir_function_impl *impl);
 
 bool nir_lower_samplers(nir_shader *shader);
-bool nir_lower_cl_images(nir_shader *shader);
+bool nir_lower_cl_images(nir_shader *shader, bool lower_image_derefs, bool lower_sampler_derefs);
 bool nir_dedup_inline_samplers(nir_shader *shader);
 bool nir_lower_ssbo(nir_shader *shader);
 
diff --git a/src/compiler/nir/nir_gather_info.c b/src/compiler/nir/nir_gather_info.c
index 8b13a699e48..e3c5ad1850a 100644
--- a/src/compiler/nir/nir_gather_info.c
+++ b/src/compiler/nir/nir_gather_info.c
@@ -1001,7 +1001,10 @@ nir_shader_gather_info(nir_shader *shader, nir_function_impl *entrypoint)
       if (var->data.bindless || var->interface_type)
          continue;
 
-      shader->info.num_textures += glsl_type_get_sampler_count(var->type);
+      if (glsl_type_is_texture(var->type))
+         shader->info.num_textures += glsl_type_get_texture_count(var->type);
+      else
+         shader->info.num_textures += glsl_type_get_sampler_count(var->type);
       shader->info.num_images += glsl_type_get_image_count(var->type);
    }
 
diff --git a/src/compiler/nir/nir_lower_cl_images.c b/src/compiler/nir/nir_lower_cl_images.c
index 8b4423fff47..2f480014193 100644
--- a/src/compiler/nir/nir_lower_cl_images.c
+++ b/src/compiler/nir/nir_lower_cl_images.c
@@ -108,17 +108,21 @@ nir_dedup_inline_samplers(nir_shader *nir)
 }
 
 bool
-nir_lower_cl_images(nir_shader *shader)
+nir_lower_cl_images(nir_shader *shader, bool lower_image_derefs, bool lower_sampler_derefs)
 {
    nir_function_impl *impl = nir_shader_get_entrypoint(shader);
 
    ASSERTED int last_loc = -1;
    int num_rd_images = 0, num_wr_images = 0;
-   nir_foreach_image_variable(var, shader) {
+   nir_foreach_variable_with_modes(var, shader, nir_var_image | nir_var_uniform) {
+      if (var->data.mode == nir_var_uniform && !glsl_type_is_texture(var->type))
+         continue;
+
       /* Assume they come in order */
       assert(var->data.location > last_loc);
       last_loc = var->data.location;
 
+      assert(glsl_type_is_image(var->type) || var->data.access & ACCESS_NON_WRITEABLE);
       if (var->data.access & ACCESS_NON_WRITEABLE)
          var->data.driver_location = num_rd_images++;
       else
@@ -155,6 +159,12 @@ nir_lower_cl_images(nir_shader *shader)
    nir_builder b;
    nir_builder_init(&b, impl);
 
+   /* don't need any lowering if we can keep the derefs */
+   if (!lower_image_derefs && !lower_sampler_derefs) {
+      nir_metadata_preserve(impl, nir_metadata_all);
+      return false;
+   }
+
    bool progress = false;
    nir_foreach_block_reverse(block, impl) {
       nir_foreach_instr_reverse_safe(instr, block) {
@@ -165,9 +175,17 @@ nir_lower_cl_images(nir_shader *shader)
                break;
 
             if (!glsl_type_is_image(deref->type) &&
+                !glsl_type_is_texture(deref->type) &&
                 !glsl_type_is_sampler(deref->type))
                break;
 
+            if (!lower_image_derefs && glsl_type_is_image(deref->type))
+               break;
+
+            if (!lower_sampler_derefs &&
+                (glsl_type_is_sampler(deref->type) || glsl_type_is_texture(deref->type)))
+               break;
+
             b.cursor = nir_instr_remove(&deref->instr);
             nir_ssa_def *loc =
                nir_imm_intN_t(&b, deref->var->data.driver_location,
@@ -178,6 +196,9 @@ nir_lower_cl_images(nir_shader *shader)
          }
 
          case nir_instr_type_tex: {
+            if (!lower_sampler_derefs)
+               break;
+
             nir_tex_instr *tex = nir_instr_as_tex(instr);
             unsigned count = 0;
             for (unsigned i = 0; i < tex->num_srcs; i++) {
@@ -242,6 +263,9 @@ nir_lower_cl_images(nir_shader *shader)
             case nir_intrinsic_image_deref_atomic_dec_wrap:
             case nir_intrinsic_image_deref_size:
             case nir_intrinsic_image_deref_samples: {
+               if (!lower_image_derefs)
+                  break;
+
                assert(intrin->src[0].is_ssa);
                b.cursor = nir_before_instr(&intrin->instr);
                /* Back-ends expect a 32-bit thing, not 64-bit */
diff --git a/src/compiler/nir/nir_lower_io.c b/src/compiler/nir/nir_lower_io.c
index 37bc6a6d377..67f68793ed8 100644
--- a/src/compiler/nir/nir_lower_io.c
+++ b/src/compiler/nir/nir_lower_io.c
@@ -925,7 +925,7 @@ static nir_ssa_def *
 build_addr_for_var(nir_builder *b, nir_variable *var,
                    nir_address_format addr_format)
 {
-   assert(var->data.mode & (nir_var_uniform | nir_var_mem_shared |
+   assert(var->data.mode & (nir_var_uniform | nir_var_image | nir_var_mem_shared |
                             nir_var_mem_task_payload |
                             nir_var_mem_global |
                             nir_var_shader_temp | nir_var_function_temp |
@@ -2208,7 +2208,7 @@ lower_explicit_io_mode_check(nir_builder *b, nir_intrinsic_instr *intrin,
 
 static bool
 nir_lower_explicit_io_impl(nir_function_impl *impl, nir_variable_mode modes,
-                           nir_address_format addr_format)
+                           bool skip_samplers, nir_address_format addr_format)
 {
    bool progress = false;
 
@@ -2225,6 +2225,9 @@ nir_lower_explicit_io_impl(nir_function_impl *impl, nir_variable_mode modes,
          case nir_instr_type_deref: {
             nir_deref_instr *deref = nir_instr_as_deref(instr);
             if (nir_deref_mode_is_in_set(deref, modes)) {
+               if (skip_samplers && (
+                  (glsl_type_is_sampler(deref->var->type) || glsl_type_is_texture(deref->var->type))))
+                  break;
                lower_explicit_io_deref(&b, deref, addr_format);
                progress = true;
             }
@@ -2355,13 +2358,13 @@ nir_lower_explicit_io_impl(nir_function_impl *impl, nir_variable_mode modes,
  */
 bool
 nir_lower_explicit_io(nir_shader *shader, nir_variable_mode modes,
-                      nir_address_format addr_format)
+                      bool skip_samplers, nir_address_format addr_format)
 {
    bool progress = false;
 
    nir_foreach_function(function, shader) {
       if (function->impl &&
-          nir_lower_explicit_io_impl(function->impl, modes, addr_format))
+          nir_lower_explicit_io_impl(function->impl, modes, skip_samplers, addr_format))
          progress = true;
    }
 
diff --git a/src/compiler/nir/nir_lower_readonly_images_to_tex.c b/src/compiler/nir/nir_lower_readonly_images_to_tex.c
index 39d45551191..a06d5e6e131 100644
--- a/src/compiler/nir/nir_lower_readonly_images_to_tex.c
+++ b/src/compiler/nir/nir_lower_readonly_images_to_tex.c
@@ -39,20 +39,20 @@ get_texture_type_for_image(const struct glsl_type *type)
                             glsl_get_sampler_result_type(type));
 }
 
-static void
-replace_image_type_with_sampler(nir_deref_instr *deref)
+static bool
+replace_image_type_with_texture(nir_deref_instr *deref)
 {
    const struct glsl_type *type = deref->type;
 
    /* If we've already chased up the deref chain this far from a different intrinsic, we're done */
-   if (glsl_type_is_texture(glsl_without_array(type)))
-      return;
+   if (!glsl_type_is_image(glsl_without_array(type)))
+      return false;
 
    deref->type = get_texture_type_for_image(type);
    deref->modes = nir_var_uniform;
    if (deref->deref_type == nir_deref_type_var) {
       type = deref->var->type;
-      if (!glsl_type_is_texture(glsl_without_array(type))) {
+      if (glsl_type_is_image(glsl_without_array(type))) {
          deref->var->type = get_texture_type_for_image(type);
          deref->var->data.mode = nir_var_uniform;
          memset(&deref->var->data.sampler, 0, sizeof(deref->var->data.sampler));
@@ -60,8 +60,10 @@ replace_image_type_with_sampler(nir_deref_instr *deref)
    } else {
       nir_deref_instr *parent = nir_deref_instr_parent(deref);
       if (parent)
-         replace_image_type_with_sampler(parent);
+         replace_image_type_with_texture(parent);
    }
+
+   return true;
 }
 
 struct readonly_image_lower_options {
@@ -69,14 +71,9 @@ struct readonly_image_lower_options {
 };
 
 static bool
-lower_readonly_image_instr(nir_builder *b, nir_instr *instr, void *context)
+lower_readonly_image_instr_intrinsic(nir_builder *b, nir_intrinsic_instr *intrin,
+                                     const struct readonly_image_lower_options *options)
 {
-   struct readonly_image_lower_options *options = (struct readonly_image_lower_options *)context;
-
-   if (instr->type != nir_instr_type_intrinsic)
-      return false;
-
-   nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
    if (intrin->intrinsic != nir_intrinsic_image_deref_load &&
        intrin->intrinsic != nir_intrinsic_image_deref_size)
       return false;
@@ -135,7 +132,7 @@ lower_readonly_image_instr(nir_builder *b, nir_instr *instr, void *context)
 
    if (options->per_variable) {
       assert(nir_deref_instr_get_variable(deref));
-      replace_image_type_with_sampler(deref);
+      replace_image_type_with_texture(deref);
    }
 
    switch (intrin->intrinsic) {
@@ -188,6 +185,38 @@ lower_readonly_image_instr(nir_builder *b, nir_instr *instr, void *context)
    return true;
 }
 
+static bool
+lower_readonly_image_instr_tex(nir_builder *b, nir_tex_instr *tex,
+                               const struct readonly_image_lower_options *options)
+{
+   int deref_idx = nir_tex_instr_src_index(tex, nir_tex_src_texture_deref);
+   if (deref_idx == -1)
+      return false;
+
+   nir_deref_instr *deref = nir_src_as_deref(tex->src[deref_idx].src);
+   if (options->per_variable) {
+      assert(nir_deref_instr_get_variable(deref));
+      return replace_image_type_with_texture(deref);
+   }
+
+   return false;
+}
+
+static bool
+lower_readonly_image_instr(nir_builder *b, nir_instr *instr, void *context)
+{
+   struct readonly_image_lower_options *options = (struct readonly_image_lower_options *)context;
+
+   switch (instr->type) {
+   case nir_instr_type_intrinsic:
+      return lower_readonly_image_instr_intrinsic(b, nir_instr_as_intrinsic(instr), options);
+   case nir_instr_type_tex:
+      return lower_readonly_image_instr_tex(b, nir_instr_as_tex(instr), options);
+   default:
+      return false;
+   }
+}
+
 /** Lowers image ops to texture ops for read-only images
  *
  * If per_variable is set:
@@ -201,8 +230,6 @@ lower_readonly_image_instr(nir_builder *b, nir_instr *instr, void *context)
 bool
 nir_lower_readonly_images_to_tex(nir_shader *shader, bool per_variable)
 {
-   assert(shader->info.stage != MESA_SHADER_KERNEL || !per_variable);
-
    struct readonly_image_lower_options options = { per_variable };
    return nir_shader_instructions_pass(shader, lower_readonly_image_instr,
                                        nir_metadata_block_index |
diff --git a/src/compiler/nir/nir_opt_algebraic.py b/src/compiler/nir/nir_opt_algebraic.py
index 026f0ee5558..3550d21e352 100644
--- a/src/compiler/nir/nir_opt_algebraic.py
+++ b/src/compiler/nir/nir_opt_algebraic.py
@@ -1053,7 +1053,7 @@ for s in [8, 16, 32, 64]:
        (('ineg', ('b2i{}'.format(s), 'a@{}'.format(s))), a),
 
        # SM5 32-bit shifts are defined to use the 5 least significant bits (or 4 bits for 16 bits)
-       (('ishl', 'a@{}'.format(s), ('iand', s - 1, b)), ('ishl', a, b)),
+#       (('ishl', 'a@{}'.format(s), ('iand', s - 1, b)), ('ishl', a, b)),
        (('ishr', 'a@{}'.format(s), ('iand', s - 1, b)), ('ishr', a, b)),
        (('ushr', 'a@{}'.format(s), ('iand', s - 1, b)), ('ushr', a, b)),
        (('ushr', 'a@{}'.format(s), ('ishl(is_used_once)', ('iand', b, 1), last_shift_bit)), ('ushr', a, ('ishl', b, last_shift_bit))),
@@ -2616,6 +2616,11 @@ late_optimizations = [
    (('~feq', ('fadd(is_used_once)', a, b), 0.0), ('feq', a, ('fneg', b))),
    (('~fneu', ('fadd(is_used_once)', a, b), 0.0), ('fneu', a, ('fneg', b))),
 
+   (('imul_high@8' , a, b), ('i2i8',  ('ishr', ('imul', ('i2i32', a), ('i2i32', b)),  8))),
+   (('umul_high@8' , a, b), ('u2u8',  ('ushr', ('imul', ('u2u32', a), ('u2u32', b)),  8))),
+   (('imul_high@16', a, b), ('i2i16', ('ishr', ('imul', ('i2i32', a), ('i2i32', b)), 16))),
+   (('umul_high@16', a, b), ('u2u16', ('ushr', ('imul', ('u2u32', a), ('u2u32', b)), 16))),
+
    # If either source must be finite, then the original (a+b) cannot produce
    # NaN due to Inf-Inf.  The patterns and the replacements produce the same
    # result if b is NaN. Therefore, the replacements are exact.
diff --git a/src/compiler/spirv/vtn_opencl.c b/src/compiler/spirv/vtn_opencl.c
index c41628fd475..5c2ec15a15f 100644
--- a/src/compiler/spirv/vtn_opencl.c
+++ b/src/compiler/spirv/vtn_opencl.c
@@ -297,7 +297,6 @@ static const struct {
    REMAP(Normalize, "normalize"),
    REMAP(Degrees, "degrees"),
    REMAP(Radians, "radians"),
-   REMAP(Rotate, "rotate"),
    REMAP(Smoothstep, "smoothstep"),
    REMAP(Step, "step"),
 
@@ -553,6 +552,8 @@ handle_special(struct vtn_builder *b, uint32_t opcode,
       if (nb->shader->options->lower_ffma32 && srcs[0]->bit_size == 32)
          break;
       return nir_ffma(nb, srcs[0], srcs[1], srcs[2]);
+   case OpenCLstd_Rotate:
+         return nir_urol(nb, srcs[0], nir_iand(nb, nir_imm_int(nb, srcs[0]->bit_size - 1), nir_u2u(nb, srcs[1], 32)));
    default:
       break;
    }
diff --git a/src/gallium/drivers/radeonsi/si_get.c b/src/gallium/drivers/radeonsi/si_get.c
index 2a5d4851c91..c2aaacb7733 100644
--- a/src/gallium/drivers/radeonsi/si_get.c
+++ b/src/gallium/drivers/radeonsi/si_get.c
@@ -1177,7 +1177,7 @@ void si_init_screen_get_functions(struct si_screen *sscreen)
       .lower_fisnormal = true,
       .lower_rotate = true,
       .lower_to_scalar = true,
-      .lower_int64_options = nir_lower_imul_2x32_64,
+      .lower_int64_options = nir_lower_imul_2x32_64 | nir_lower_imul_high64,
       .has_sdot_4x8 = sscreen->info.has_accelerated_dot_product,
       .has_sudot_4x8 = sscreen->info.has_accelerated_dot_product && sscreen->info.gfx_level >= GFX11,
       .has_udot_4x8 = sscreen->info.has_accelerated_dot_product,
diff --git a/src/gallium/drivers/radeonsi/si_shader_nir.c b/src/gallium/drivers/radeonsi/si_shader_nir.c
index 04722413240..f3cf8de5913 100644
--- a/src/gallium/drivers/radeonsi/si_shader_nir.c
+++ b/src/gallium/drivers/radeonsi/si_shader_nir.c
@@ -342,7 +342,7 @@ char *si_finalize_nir(struct pipe_screen *screen, void *nirptr)
 
    nir_lower_io_passes(nir);
 
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_shared, nir_address_format_32bit_offset);
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_shared, false, nir_address_format_32bit_offset);
 
    /* Remove dead derefs, so that we can remove uniforms. */
    NIR_PASS_V(nir, nir_opt_dce);
@@ -350,6 +350,7 @@ char *si_finalize_nir(struct pipe_screen *screen, void *nirptr)
    /* Remove uniforms because those should have been lowered to UBOs already. */
    nir_foreach_variable_with_modes_safe(var, nir, nir_var_uniform) {
       if (!glsl_type_get_image_count(var->type) &&
+          !glsl_type_get_texture_count(var->type) &&
           !glsl_type_get_sampler_count(var->type))
          exec_node_remove(&var->node);
    }
diff --git a/src/gallium/frontends/clover/nir/invocation.cpp b/src/gallium/frontends/clover/nir/invocation.cpp
index 907f79bce08..5fc80b9dbd1 100644
--- a/src/gallium/frontends/clover/nir/invocation.cpp
+++ b/src/gallium/frontends/clover/nir/invocation.cpp
@@ -359,7 +359,7 @@ binary clover::nir::spirv_to_nir(const binary &mod, const device &dev,
                                              nir->constant_data_size,
                                              nir_var_mem_constant);
       }
-      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_constant,
+      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_constant, false,
                  spirv_options.constant_addr_format);
 
       auto args = sym.args;
@@ -376,24 +376,24 @@ binary clover::nir::spirv_to_nir(const binary &mod, const device &dev,
 
       NIR_PASS_V(nir, nir_opt_deref);
       NIR_PASS_V(nir, nir_lower_readonly_images_to_tex, false);
-      NIR_PASS_V(nir, nir_lower_cl_images);
+      NIR_PASS_V(nir, nir_lower_cl_images, true, true);
       NIR_PASS_V(nir, nir_lower_memcpy);
 
       /* use offsets for kernel inputs (uniform) */
-      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_uniform,
+      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_uniform, false,
                  nir->info.cs.ptr_size == 64 ?
                  nir_address_format_32bit_offset_as_64bit :
                  nir_address_format_32bit_offset);
 
-      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_constant,
+      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_constant, false,
                  spirv_options.constant_addr_format);
-      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_shared,
+      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_shared, false,
                  spirv_options.shared_addr_format);
 
-      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_function_temp,
+      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_function_temp, false,
                  spirv_options.temp_addr_format);
 
-      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_global,
+      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_global, false,
                  spirv_options.global_addr_format);
 
       struct nir_remove_dead_variables_options remove_dead_variables_options = {};
diff --git a/src/gallium/frontends/lavapipe/lvp_pipeline.c b/src/gallium/frontends/lavapipe/lvp_pipeline.c
index 2efd36c9c45..22cad864c76 100644
--- a/src/gallium/frontends/lavapipe/lvp_pipeline.c
+++ b/src/gallium/frontends/lavapipe/lvp_pipeline.c
@@ -460,20 +460,20 @@ lvp_shader_compile_to_ir(struct lvp_pipeline *pipeline,
    NIR_PASS_V(nir, nir_split_var_copies);
    NIR_PASS_V(nir, nir_lower_global_vars_to_local);
 
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_push_const,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_push_const, false,
               nir_address_format_32bit_offset);
 
    NIR_PASS_V(nir, nir_lower_explicit_io,
-              nir_var_mem_ubo | nir_var_mem_ssbo,
+              nir_var_mem_ubo | nir_var_mem_ssbo, false,
               nir_address_format_32bit_index_offset);
 
    NIR_PASS_V(nir, nir_lower_explicit_io,
-              nir_var_mem_global,
+              nir_var_mem_global, false,
               nir_address_format_64bit_global);
 
    if (nir->info.stage == MESA_SHADER_COMPUTE) {
       NIR_PASS_V(nir, nir_lower_vars_to_explicit_types, nir_var_mem_shared, shared_var_info);
-      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_shared, nir_address_format_32bit_offset);
+      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_shared, false, nir_address_format_32bit_offset);
    }
 
    NIR_PASS_V(nir, nir_remove_dead_variables, nir_var_shader_temp, NULL);
diff --git a/src/gallium/frontends/rusticl/core/device.rs b/src/gallium/frontends/rusticl/core/device.rs
index b8f1a2d9dff..08b1c130dab 100644
--- a/src/gallium/frontends/rusticl/core/device.rs
+++ b/src/gallium/frontends/rusticl/core/device.rs
@@ -536,8 +536,7 @@ impl Device {
     }
 
     pub fn const_max_size(&self) -> cl_ulong {
-        self.screen
-            .param(pipe_cap::PIPE_CAP_MAX_SHADER_BUFFER_SIZE_UINT) as u64
+        self.shader_param(pipe_shader_cap::PIPE_SHADER_CAP_MAX_CONST_BUFFER0_SIZE) as cl_ulong
     }
 
     pub fn device_type(&self, internal: bool) -> cl_device_type {
@@ -742,6 +741,14 @@ impl Device {
         self.screen.param(pipe_cap::PIPE_CAP_SHAREABLE_SHADERS) == 1
     }
 
+    pub fn images_as_deref(&self) -> bool {
+        self.screen.param(pipe_cap::PIPE_CAP_NIR_IMAGES_AS_DEREF) == 1
+    }
+
+    pub fn samplers_as_deref(&self) -> bool {
+        self.screen.param(pipe_cap::PIPE_CAP_NIR_SAMPLERS_AS_DEREF) == 1
+    }
+
     pub fn helper_ctx(&self) -> impl HelperContextWrapper + '_ {
         HelperContext {
             lock: self.helper_ctx.lock().unwrap(),
diff --git a/src/gallium/frontends/rusticl/core/kernel.rs b/src/gallium/frontends/rusticl/core/kernel.rs
index fdf26a3ae2f..3e892197b8d 100644
--- a/src/gallium/frontends/rusticl/core/kernel.rs
+++ b/src/gallium/frontends/rusticl/core/kernel.rs
@@ -68,7 +68,10 @@ pub struct KernelArg {
     spirv: spirv::SPIRVKernelArg,
     pub kind: KernelArgType,
     pub size: usize,
+    /// The offset into the input buffer
     pub offset: usize,
+    /// The actual binding slot
+    pub binding: u32,
     pub dead: bool,
 }
 
@@ -127,6 +130,7 @@ impl KernelArg {
                 // we'll update it later in the 2nd pass
                 kind: kind,
                 offset: 0,
+                binding: 0,
                 dead: true,
             });
         }
@@ -143,6 +147,7 @@ impl KernelArg {
         ) {
             if let Some(arg) = args.get_mut(var.data.location as usize) {
                 arg.offset = var.data.driver_location as usize;
+                arg.binding = var.data.binding;
                 arg.dead = false;
             } else {
                 internal_args
@@ -159,6 +164,7 @@ impl KernelArg {
         bin.append(&mut self.spirv.serialize());
         bin.extend_from_slice(&self.size.to_ne_bytes());
         bin.extend_from_slice(&self.offset.to_ne_bytes());
+        bin.extend_from_slice(&self.binding.to_ne_bytes());
         bin.extend_from_slice(&(self.dead as u8).to_ne_bytes());
         bin.extend_from_slice(&(self.kind as u8).to_ne_bytes());
 
@@ -169,6 +175,7 @@ impl KernelArg {
         let spirv = spirv::SPIRVKernelArg::deserialize(bin)?;
         let size = read_ne_usize(bin);
         let offset = read_ne_usize(bin);
+        let binding = read_ne_u32(bin);
         let dead = read_ne_u8(bin) == 1;
 
         let kind = match read_ne_u8(bin) {
@@ -188,6 +195,7 @@ impl KernelArg {
             kind: kind,
             size: size,
             offset: offset,
+            binding: binding,
             dead: dead,
         })
     }
@@ -452,6 +460,8 @@ extern "C" fn can_remove_var(var: *mut nir_variable, _: *mut c_void) -> bool {
     unsafe {
         let var = var.as_ref().unwrap();
         !glsl_type_is_image(var.type_)
+            && !glsl_type_is_texture(var.type_)
+            && !glsl_type_is_sampler(var.type_)
     }
 }
 
@@ -523,8 +533,12 @@ fn lower_and_optimize_nir_late(
         }
     }
 
-    nir.pass1(nir_lower_readonly_images_to_tex, false);
-    nir.pass0(nir_lower_cl_images);
+    nir.pass1(nir_lower_readonly_images_to_tex, true);
+    nir.pass2(
+        nir_lower_cl_images,
+        !dev.images_as_deref(),
+        !dev.samplers_as_deref(),
+    );
 
     nir.reset_scratch_size();
     nir.pass2(
@@ -581,30 +595,30 @@ fn lower_and_optimize_nir_late(
     compute_options.set_has_base_global_invocation_id(true);
     nir.pass1(nir_lower_compute_system_values, &compute_options);
     nir.pass1(nir_shader_gather_info, nir.entrypoint());
-
-    if nir.num_images() > 0 {
+    if nir.num_images() > 0 || nir.num_textures() > 0 {
+        let count = nir.num_images() + nir.num_textures();
         res.push(InternalKernelArg {
             kind: InternalKernelArgType::FormatArray,
             offset: 0,
-            size: 2 * nir.num_images() as usize,
+            size: 2 * count as usize,
         });
 
         res.push(InternalKernelArg {
             kind: InternalKernelArgType::OrderArray,
             offset: 0,
-            size: 2 * nir.num_images() as usize,
+            size: 2 * count as usize,
         });
 
         lower_state.format_arr = nir.add_var(
             nir_variable_mode::nir_var_uniform,
-            unsafe { glsl_array_type(glsl_int16_t_type(), nir.num_images() as u32, 2) },
+            unsafe { glsl_array_type(glsl_int16_t_type(), count as u32, 2) },
             args.len() + res.len() - 2,
             "image_formats",
         );
 
         lower_state.order_arr = nir.add_var(
             nir_variable_mode::nir_var_uniform,
-            unsafe { glsl_array_type(glsl_int16_t_type(), nir.num_images() as u32, 2) },
+            unsafe { glsl_array_type(glsl_int16_t_type(), count as u32, 2) },
             args.len() + res.len() - 1,
             "image_orders",
         );
@@ -643,18 +657,20 @@ fn lower_and_optimize_nir_late(
         shared_address_format = nir_address_format::nir_address_format_32bit_offset_as_64bit;
     }
 
-    nir.pass2(
+    nir.pass3(
         nir_lower_explicit_io,
         nir_variable_mode::nir_var_mem_global | nir_variable_mode::nir_var_mem_constant,
+        false,
         global_address_format,
     );
 
     nir.pass1(rusticl_lower_intrinsics, &mut lower_state);
-    nir.pass2(
+    nir.pass3(
         nir_lower_explicit_io,
         nir_variable_mode::nir_var_mem_shared
             | nir_variable_mode::nir_var_function_temp
             | nir_variable_mode::nir_var_uniform,
+        dev.samplers_as_deref(),
         shared_address_format,
     );
 
@@ -803,7 +819,7 @@ fn extract<'a, const S: usize>(buf: &'a mut &[u8]) -> &'a [u8; S] {
 }
 
 fn optimize_local_size(d: &Device, grid: &mut [u32; 3], block: &mut [u32; 3]) {
-    let mut threads = d.max_threads_per_block() as u32;
+    let mut threads = d.max_threads_per_block() as u32 / 4;
     let dim_threads = d.max_block_sizes();
     let subgroups = d.subgroups();
 
@@ -939,10 +955,11 @@ impl Kernel {
                             (&mut tex_formats, &mut tex_orders)
                         };
 
-                        assert!(arg.offset >= formats.len());
+                        let binding = arg.binding as usize;
+                        assert!(binding >= formats.len());
 
-                        formats.resize(arg.offset, 0);
-                        orders.resize(arg.offset, 0);
+                        formats.resize(binding, 0);
+                        orders.resize(binding, 0);
 
                         formats.push(mem.image_format.image_channel_data_type as u16);
                         orders.push(mem.image_format.image_channel_order as u16);
@@ -1002,7 +1019,7 @@ impl Kernel {
                     let buf = Arc::new(
                         q.device
                             .screen
-                            .resource_create_buffer(printf_size, ResourceType::Normal)
+                            .resource_create_buffer(printf_size, ResourceType::Staging)
                             .unwrap(),
                     );
 
diff --git a/src/gallium/frontends/rusticl/mesa/compiler/nir.rs b/src/gallium/frontends/rusticl/mesa/compiler/nir.rs
index c9d1c7815dd..347b3c95c2a 100644
--- a/src/gallium/frontends/rusticl/mesa/compiler/nir.rs
+++ b/src/gallium/frontends/rusticl/mesa/compiler/nir.rs
@@ -174,6 +174,10 @@ impl NirShader {
         unsafe { (*self.nir.as_ptr()).info.num_images }
     }
 
+    pub fn num_textures(&self) -> u8 {
+        unsafe { (*self.nir.as_ptr()).info.num_textures }
+    }
+
     pub fn reset_scratch_size(&self) {
         unsafe {
             (*self.nir.as_ptr()).scratch_size = 0;
diff --git a/src/gallium/frontends/rusticl/mesa/pipe/screen.rs b/src/gallium/frontends/rusticl/mesa/pipe/screen.rs
index 37bd1b88dbb..c8b9a19f18f 100644
--- a/src/gallium/frontends/rusticl/mesa/pipe/screen.rs
+++ b/src/gallium/frontends/rusticl/mesa/pipe/screen.rs
@@ -93,7 +93,7 @@ impl PipeScreen {
                 (*self.screen).context_create.unwrap()(
                     self.screen,
                     ptr::null_mut(),
-                    PIPE_CONTEXT_COMPUTE_ONLY,
+                    0, //PIPE_CONTEXT_COMPUTE_ONLY,
                 )
             },
             self,
diff --git a/src/gallium/frontends/rusticl/rusticl_nir.c b/src/gallium/frontends/rusticl/rusticl_nir.c
index 8e9e51050b6..61a55d5c6d5 100644
--- a/src/gallium/frontends/rusticl/rusticl_nir.c
+++ b/src/gallium/frontends/rusticl/rusticl_nir.c
@@ -39,8 +39,19 @@ rusticl_lower_intrinsics_instr(
         }
 
         val = intrins->src[0].ssa;
+
+        nir_address_format addr_format =
+            val->bit_size == 64
+                ? nir_address_format_32bit_offset_as_64bit
+                : nir_address_format_32bit_offset;
+
+        if (val->parent_instr->type == nir_instr_type_deref) {
+            nir_deref_instr *deref = nir_instr_as_deref(val->parent_instr);
+            val = nir_explicit_io_address_from_deref(b, deref, NULL, addr_format);
+        }
+
         // we put write images after read images
-        if (nir_intrinsic_access(intrins) & ACCESS_NON_WRITEABLE) {
+        if (glsl_type_is_image(var->type)) {
             val = nir_iadd_imm(b, val, b->shader->info.num_textures);
         }
 
diff --git a/src/gallium/targets/rusticl/meson.build b/src/gallium/targets/rusticl/meson.build
index a968dee52db..c3a9cf27eb0 100644
--- a/src/gallium/targets/rusticl/meson.build
+++ b/src/gallium/targets/rusticl/meson.build
@@ -45,6 +45,7 @@ librusticl = shared_library(
     driver_iris,
     driver_nouveau,
     driver_panfrost,
+    driver_radeonsi,
     driver_swrast,
     idep_nir,
   ],
diff --git a/src/imagination/rogue/rogue_nir.c b/src/imagination/rogue/rogue_nir.c
index 9c6be45cc54..49194d5f42f 100644
--- a/src/imagination/rogue/rogue_nir.c
+++ b/src/imagination/rogue/rogue_nir.c
@@ -130,7 +130,7 @@ bool rogue_nir_passes(struct rogue_build_ctx *ctx,
    /* Additional I/O lowering. */
    NIR_PASS_V(nir,
               nir_lower_explicit_io,
-              nir_var_mem_ubo,
+              nir_var_mem_ubo, false,
               spirv_options.ubo_addr_format);
    NIR_PASS_V(nir, rogue_nir_lower_io, NULL);
 
diff --git a/src/intel/compiler/brw_kernel.c b/src/intel/compiler/brw_kernel.c
index fd5f4e62180..53a1e64e6c5 100644
--- a/src/intel/compiler/brw_kernel.c
+++ b/src/intel/compiler/brw_kernel.c
@@ -427,15 +427,15 @@ brw_kernel_from_spirv(struct brw_compiler *compiler,
 
    NIR_PASS_V(nir, nir_lower_memcpy);
 
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_constant,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_constant, false,
               nir_address_format_64bit_global);
 
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_uniform,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_uniform, false,
               nir_address_format_32bit_offset_as_64bit);
 
    NIR_PASS_V(nir, nir_lower_explicit_io,
               nir_var_shader_temp | nir_var_function_temp |
-              nir_var_mem_shared | nir_var_mem_global,
+              nir_var_mem_shared | nir_var_mem_global, false,
               nir_address_format_62bit_generic);
 
    NIR_PASS_V(nir, nir_lower_frexp);
diff --git a/src/intel/compiler/brw_mesh.cpp b/src/intel/compiler/brw_mesh.cpp
index dfae5ce6370..3dd549eb4f6 100644
--- a/src/intel/compiler/brw_mesh.cpp
+++ b/src/intel/compiler/brw_mesh.cpp
@@ -163,7 +163,7 @@ brw_nir_lower_tue_outputs(nir_shader *nir, brw_tue_map *map)
    NIR_PASS(_, nir, nir_lower_vars_to_explicit_types,
             nir_var_mem_task_payload, shared_type_info);
    NIR_PASS(_, nir, nir_lower_explicit_io,
-            nir_var_mem_task_payload, nir_address_format_32bit_offset);
+            nir_var_mem_task_payload, false, nir_address_format_32bit_offset);
 
    map->size_dw = ALIGN(DIV_ROUND_UP(nir->info.task_payload_size, 4), 8);
 }
@@ -365,7 +365,7 @@ brw_nir_lower_tue_inputs(nir_shader *nir, const brw_tue_map *map)
       nir->info.task_payload_size = 0;
    }
 
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_task_payload,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_task_payload, false,
             nir_address_format_32bit_offset);
 }
 
diff --git a/src/intel/compiler/brw_nir.c b/src/intel/compiler/brw_nir.c
index 760b661f5ba..9d139a3064d 100644
--- a/src/intel/compiler/brw_nir.c
+++ b/src/intel/compiler/brw_nir.c
@@ -1267,7 +1267,7 @@ brw_postprocess_nir(nir_shader *nir, const struct brw_compiler *compiler,
    if (is_scalar && nir_shader_has_local_variables(nir)) {
       OPT(nir_lower_vars_to_explicit_types, nir_var_function_temp,
           glsl_get_natural_size_align_bytes);
-      OPT(nir_lower_explicit_io, nir_var_function_temp,
+      OPT(nir_lower_explicit_io, nir_var_function_temp, false,
           nir_address_format_32bit_offset);
       brw_nir_optimize(nir, compiler, is_scalar, false);
    }
diff --git a/src/intel/compiler/brw_nir_rt.c b/src/intel/compiler/brw_nir_rt.c
index b124928b293..b18248391ab 100644
--- a/src/intel/compiler/brw_nir_rt.c
+++ b/src/intel/compiler/brw_nir_rt.c
@@ -218,7 +218,7 @@ lower_rt_io_and_scratch(nir_shader *nir)
    NIR_PASS_V(nir, nir_lower_explicit_io,
               nir_var_function_temp |
               nir_var_mem_constant |
-              nir_var_ray_hit_attrib,
+              nir_var_ray_hit_attrib, false,
               nir_address_format_64bit_global);
 }
 
diff --git a/src/intel/vulkan/anv_pipeline.c b/src/intel/vulkan/anv_pipeline.c
index 56b5b9c1824..2c3cfa5bdf0 100644
--- a/src/intel/vulkan/anv_pipeline.c
+++ b/src/intel/vulkan/anv_pipeline.c
@@ -918,9 +918,9 @@ anv_pipeline_lower_nir(struct anv_pipeline *pipeline,
 
    NIR_PASS(_, nir, brw_nir_lower_storage_image, compiler->devinfo);
 
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_global,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_global, false,
             nir_address_format_64bit_global);
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_push_const,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_push_const, false,
             nir_address_format_32bit_offset);
 
    NIR_PASS(_, nir, brw_nir_lower_ray_queries, &pdevice->info);
@@ -933,10 +933,10 @@ anv_pipeline_lower_nir(struct anv_pipeline *pipeline,
               pdevice, pipeline->device->robust_buffer_access,
               layout, &stage->bind_map);
 
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ubo,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ubo, false,
             anv_nir_ubo_addr_format(pdevice,
                pipeline->device->robust_buffer_access));
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ssbo,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ssbo, false,
             anv_nir_ssbo_addr_format(pdevice,
                pipeline->device->robust_buffer_access));
 
@@ -981,7 +981,7 @@ anv_pipeline_lower_nir(struct anv_pipeline *pipeline,
       }
 
       NIR_PASS(_, nir, nir_lower_explicit_io,
-               nir_var_mem_shared, nir_address_format_32bit_offset);
+               nir_var_mem_shared, false, nir_address_format_32bit_offset);
 
       if (nir->info.zero_initialize_shared_memory &&
           nir->info.shared_size > 0) {
diff --git a/src/intel/vulkan/anv_pipeline_cache.c b/src/intel/vulkan/anv_pipeline_cache.c
index 0c162a41065..ff316784549 100644
--- a/src/intel/vulkan/anv_pipeline_cache.c
+++ b/src/intel/vulkan/anv_pipeline_cache.c
@@ -473,7 +473,7 @@ anv_load_fp64_shader(struct anv_device *device)
    NIR_PASS_V(nir, nir_opt_peephole_select, 1, false, false);
    NIR_PASS_V(nir, nir_opt_dce);
 
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_function_temp,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_function_temp, false,
               nir_address_format_62bit_generic);
 
    anv_device_upload_nir(device, device->internal_cache,
diff --git a/src/intel/vulkan_hasvk/anv_pipeline.c b/src/intel/vulkan_hasvk/anv_pipeline.c
index a0be151af6b..1fdd3bb6e1c 100644
--- a/src/intel/vulkan_hasvk/anv_pipeline.c
+++ b/src/intel/vulkan_hasvk/anv_pipeline.c
@@ -642,9 +642,9 @@ anv_pipeline_lower_nir(struct anv_pipeline *pipeline,
 
    NIR_PASS(_, nir, brw_nir_lower_storage_image, compiler->devinfo);
 
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_global,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_global, false,
             nir_address_format_64bit_global);
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_push_const,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_push_const, false,
             nir_address_format_32bit_offset);
 
    NIR_PASS(_, nir, brw_nir_lower_ray_queries, &pdevice->info);
@@ -654,10 +654,10 @@ anv_pipeline_lower_nir(struct anv_pipeline *pipeline,
               pdevice, pipeline->device->robust_buffer_access,
               layout, &stage->bind_map);
 
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ubo,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ubo, false,
             anv_nir_ubo_addr_format(pdevice,
                pipeline->device->robust_buffer_access));
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ssbo,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ssbo, false,
             anv_nir_ssbo_addr_format(pdevice,
                pipeline->device->robust_buffer_access));
 
@@ -702,7 +702,7 @@ anv_pipeline_lower_nir(struct anv_pipeline *pipeline,
       }
 
       NIR_PASS(_, nir, nir_lower_explicit_io,
-               nir_var_mem_shared, nir_address_format_32bit_offset);
+               nir_var_mem_shared, false, nir_address_format_32bit_offset);
 
       if (nir->info.zero_initialize_shared_memory &&
           nir->info.shared_size > 0) {
diff --git a/src/mesa/state_tracker/st_glsl_to_nir.cpp b/src/mesa/state_tracker/st_glsl_to_nir.cpp
index a994e65f81b..ee61446bc30 100644
--- a/src/mesa/state_tracker/st_glsl_to_nir.cpp
+++ b/src/mesa/state_tracker/st_glsl_to_nir.cpp
@@ -419,7 +419,7 @@ st_nir_preprocess(struct st_context *st, struct gl_program *prog,
       NIR_PASS_V(prog->nir, nir_lower_vars_to_explicit_types,
                  nir_var_mem_shared, shared_type_info);
       NIR_PASS_V(prog->nir, nir_lower_explicit_io,
-                 nir_var_mem_shared, nir_address_format_32bit_offset);
+                 nir_var_mem_shared, false, nir_address_format_32bit_offset);
    }
 
    /* Do a round of constant folding to clean up address calculations */
diff --git a/src/microsoft/clc/clc_compiler.c b/src/microsoft/clc/clc_compiler.c
index 92a97a42e7f..b97019cedf3 100644
--- a/src/microsoft/clc/clc_compiler.c
+++ b/src/microsoft/clc/clc_compiler.c
@@ -1020,10 +1020,10 @@ clc_spirv_to_dxil(struct clc_libclc *lib,
    NIR_PASS_V(nir, split_unaligned_loads_stores);
 
    assert(nir->info.cs.ptr_size == 64);
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ssbo,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ssbo, false,
               nir_address_format_32bit_index_offset_pack64);
    NIR_PASS_V(nir, nir_lower_explicit_io,
-              nir_var_mem_shared | nir_var_function_temp | nir_var_uniform,
+              nir_var_mem_shared | nir_var_function_temp | nir_var_uniform, false,
               nir_address_format_32bit_offset_as_64bit);
 
    NIR_PASS_V(nir, nir_lower_system_values);
@@ -1078,7 +1078,7 @@ clc_spirv_to_dxil(struct clc_libclc *lib,
 
    NIR_PASS_V(nir, clc_nir_lower_kernel_input_loads, inputs_var);
    NIR_PASS_V(nir, split_unaligned_loads_stores);
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ubo,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ubo, false,
               nir_address_format_32bit_index_offset);
    NIR_PASS_V(nir, clc_nir_lower_system_values, work_properties_var);
    NIR_PASS_V(nir, dxil_nir_lower_loads_stores_to_dxil);
diff --git a/src/microsoft/spirv_to_dxil/dxil_spirv_nir.c b/src/microsoft/spirv_to_dxil/dxil_spirv_nir.c
index a1182cee0dc..daac91c5c2a 100644
--- a/src/microsoft/spirv_to_dxil/dxil_spirv_nir.c
+++ b/src/microsoft/spirv_to_dxil/dxil_spirv_nir.c
@@ -652,7 +652,7 @@ dxil_spirv_nir_passes(nir_shader *nir,
               NULL);
 
    uint32_t push_constant_size = 0;
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_push_const,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_push_const, false,
               nir_address_format_32bit_offset);
    NIR_PASS_V(nir, dxil_spirv_nir_lower_load_push_constant,
               nir_address_format_32bit_index_offset,
@@ -660,14 +660,14 @@ dxil_spirv_nir_passes(nir_shader *nir,
               conf->push_constant_cbv.base_shader_register,
               &push_constant_size);
 
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ubo | nir_var_mem_ssbo,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ubo | nir_var_mem_ssbo, false,
               nir_address_format_32bit_index_offset);
 
    if (!nir->info.shared_memory_explicit_layout) {
       NIR_PASS_V(nir, nir_lower_vars_to_explicit_types, nir_var_mem_shared,
                  shared_var_info);
    }
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_shared,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_shared, false,
       nir_address_format_32bit_offset_as_64bit);
 
    NIR_PASS_V(nir, nir_lower_clip_cull_distance_arrays);
diff --git a/src/nouveau/codegen/nv50_ir_from_nir.cpp b/src/nouveau/codegen/nv50_ir_from_nir.cpp
index 4550acbf180..7f144f4265e 100644
--- a/src/nouveau/codegen/nv50_ir_from_nir.cpp
+++ b/src/nouveau/codegen/nv50_ir_from_nir.cpp
@@ -3337,7 +3337,7 @@ Converter::run()
 
    /* codegen assumes vec4 alignment for memory */
    NIR_PASS_V(nir, nir_lower_vars_to_explicit_types, nir_var_function_temp, function_temp_type_info);
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_function_temp, nir_address_format_32bit_offset);
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_function_temp, false, nir_address_format_32bit_offset);
    NIR_PASS_V(nir, nir_remove_dead_variables, nir_var_function_temp, NULL);
 
    NIR_PASS_V(nir, nir_lower_io, nir_var_shader_in | nir_var_shader_out,
diff --git a/src/panfrost/vulkan/panvk_vX_shader.c b/src/panfrost/vulkan/panvk_vX_shader.c
index fde8445ca98..049d1e7e9c9 100644
--- a/src/panfrost/vulkan/panvk_vX_shader.c
+++ b/src/panfrost/vulkan/panvk_vX_shader.c
@@ -316,12 +316,12 @@ panvk_per_arch(shader_create)(struct panvk_device *dev,
    NIR_PASS_V(nir, panvk_per_arch(nir_lower_descriptors),
               dev, layout, &shader->has_img_access);
 
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ubo,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ubo, false,
               nir_address_format_32bit_index_offset);
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ssbo,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ssbo, false,
               spirv_options.ssbo_addr_format);
    NIR_PASS_V(nir, nir_lower_explicit_io,
-              nir_var_mem_push_const,
+              nir_var_mem_push_const, false,
               nir_address_format_32bit_offset);
 
    if (gl_shader_stage_uses_workgroup(stage)) {
@@ -332,7 +332,7 @@ panvk_per_arch(shader_create)(struct panvk_device *dev,
       }
 
       NIR_PASS_V(nir, nir_lower_explicit_io,
-                 nir_var_mem_shared,
+                 nir_var_mem_shared, false,
                  nir_address_format_32bit_offset);
    }
 
