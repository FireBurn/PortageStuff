diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 83a853742ef..13d6e993990 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -3500,6 +3500,10 @@ visit_alu_instr(isel_context* ctx, nir_alu_instr* instr)
       bld.copy(Definition(dst), get_alu_src(ctx, instr->src[0]));
       emit_split_vector(ctx, dst, instr->op == nir_op_unpack_64_4x16 ? 4 : 2);
       break;
+   case nir_op_unpack_32_4x8:
+      bld.copy(Definition(dst), get_alu_src(ctx, instr->src[0]));
+      emit_split_vector(ctx, dst, 4);
+      break;
    case nir_op_pack_64_2x32_split: {
       Temp src0 = get_alu_src(ctx, instr->src[0]);
       Temp src1 = get_alu_src(ctx, instr->src[1]);
diff --git a/src/amd/llvm/ac_llvm_build.c b/src/amd/llvm/ac_llvm_build.c
index 8f08ca47c50..3ff67ab1a37 100644
--- a/src/amd/llvm/ac_llvm_build.c
+++ b/src/amd/llvm/ac_llvm_build.c
@@ -83,6 +83,7 @@ void ac_llvm_context_init(struct ac_llvm_context *ctx, struct ac_llvm_compiler *
    ctx->f16 = LLVMHalfTypeInContext(ctx->context);
    ctx->f32 = LLVMFloatTypeInContext(ctx->context);
    ctx->f64 = LLVMDoubleTypeInContext(ctx->context);
+   ctx->v4i8 = LLVMVectorType(ctx->i8, 4);
    ctx->v2i16 = LLVMVectorType(ctx->i16, 2);
    ctx->v4i16 = LLVMVectorType(ctx->i16, 4);
    ctx->v2f16 = LLVMVectorType(ctx->f16, 2);
diff --git a/src/amd/llvm/ac_llvm_build.h b/src/amd/llvm/ac_llvm_build.h
index 823005ebb37..0983f1ad757 100644
--- a/src/amd/llvm/ac_llvm_build.h
+++ b/src/amd/llvm/ac_llvm_build.h
@@ -96,6 +96,7 @@ struct ac_llvm_context {
    LLVMTypeRef f16;
    LLVMTypeRef f32;
    LLVMTypeRef f64;
+   LLVMTypeRef v4i8;
    LLVMTypeRef v2i16;
    LLVMTypeRef v4i16;
    LLVMTypeRef v2f16;
diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index b89fe706516..e0e1668c521 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -556,6 +556,7 @@ static bool visit_alu(struct ac_nir_context *ctx, const nir_alu_instr *instr)
    case nir_op_vec5:
    case nir_op_vec8:
    case nir_op_vec16:
+   case nir_op_unpack_32_4x8:
    case nir_op_unpack_32_2x16:
    case nir_op_unpack_64_2x32:
    case nir_op_unpack_64_4x16:
@@ -683,6 +684,23 @@ static bool visit_alu(struct ac_nir_context *ctx, const nir_alu_instr *instr)
    case nir_op_fmul:
       src[0] = ac_to_float(&ctx->ac, src[0]);
       src[1] = ac_to_float(&ctx->ac, src[1]);
+      if (nir_src_is_const(instr->src[0].src) && nir_src_as_float(instr->src[0].src) == 1.0) {
+         if (ac_get_type_size(def_type) == 8) {
+            result = ac_build_intrinsic(&ctx->ac, "llvm.canonicalize.f64", ctx->ac.f64, &src[1], 1, AC_FUNC_ATTR_READNONE);
+            break;
+         } else if (ac_get_type_size(def_type) == 4) {
+            result = ac_build_intrinsic(&ctx->ac, "llvm.canonicalize.f32", ctx->ac.f32, &src[1], 1, AC_FUNC_ATTR_READNONE);
+            break;
+         }
+      } else if (nir_src_is_const(instr->src[1].src) && nir_src_as_float(instr->src[1].src) == 1.0) {
+         if (ac_get_type_size(def_type) == 8) {
+            result = ac_build_intrinsic(&ctx->ac, "llvm.canonicalize.f64", ctx->ac.f64, &src[0], 1, AC_FUNC_ATTR_READNONE);
+            break;
+         } else if (ac_get_type_size(def_type) == 4) {
+            result = ac_build_intrinsic(&ctx->ac, "llvm.canonicalize.f32", ctx->ac.f32, &src[0], 1, AC_FUNC_ATTR_READNONE);
+            break;
+         }
+      }
       result = LLVMBuildFMul(ctx->ac.builder, src[0], src[1], "");
       break;
    case nir_op_fmulz:
@@ -1239,6 +1257,9 @@ static bool visit_alu(struct ac_nir_context *ctx, const nir_alu_instr *instr)
       break;
    }
 
+   case nir_op_unpack_32_4x8:
+      result = LLVMBuildBitCast(ctx->ac.builder, src[0], ctx->ac.v4i8, "");
+      break;
    case nir_op_unpack_32_2x16: {
       result = LLVMBuildBitCast(ctx->ac.builder, src[0],
             ctx->ac.v2i16, "");
@@ -2307,34 +2328,17 @@ static LLVMValueRef visit_load_ubo_buffer(struct ac_nir_context *ctx, nir_intrin
    LLVMValueRef offset = get_src(ctx, instr->src[1]);
    int num_components = instr->num_components;
 
+   assert(instr->dest.ssa.bit_size >= 32 && instr->dest.ssa.bit_size % 32 == 0);
+
    if (ctx->abi->load_ubo)
       rsrc = ctx->abi->load_ubo(ctx->abi, rsrc);
 
-   /* Convert to a scalar 32-bit load. */
+   /* Convert to a 32-bit load. */
    if (instr->dest.ssa.bit_size == 64)
       num_components *= 2;
-   else if (instr->dest.ssa.bit_size == 16)
-      num_components = DIV_ROUND_UP(num_components, 2);
-   else if (instr->dest.ssa.bit_size == 8)
-      num_components = DIV_ROUND_UP(num_components, 4);
-
-   ret =
-      ac_build_buffer_load(&ctx->ac, rsrc, num_components, NULL, offset, NULL,
-                           ctx->ac.f32, 0, true, true);
-
-   /* Convert to the original type. */
-   if (instr->dest.ssa.bit_size == 64) {
-      ret = LLVMBuildBitCast(ctx->ac.builder, ret,
-                             LLVMVectorType(ctx->ac.i64, num_components / 2), "");
-   } else if (instr->dest.ssa.bit_size == 16) {
-      ret = LLVMBuildBitCast(ctx->ac.builder, ret,
-                             LLVMVectorType(ctx->ac.i16, num_components * 2), "");
-   } else if (instr->dest.ssa.bit_size == 8) {
-      ret = LLVMBuildBitCast(ctx->ac.builder, ret,
-                             LLVMVectorType(ctx->ac.i8, num_components * 4), "");
-   }
 
-   ret = ac_trim_vector(&ctx->ac, ret, instr->num_components);
+   ret = ac_build_buffer_load(&ctx->ac, rsrc, num_components, NULL, offset, NULL,
+                              ctx->ac.f32, 0, true, true);
    ret = LLVMBuildBitCast(ctx->ac.builder, ret, get_def_type(ctx, &instr->dest.ssa), "");
 
    return exit_waterfall(ctx, &wctx, ret);
diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index 7506d7af6c2..70ae28a09be 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -3877,6 +3877,12 @@ radv_postprocess_nir(struct radv_pipeline *pipeline,
       nir_shader_gather_info(stage->nir, nir_shader_get_entrypoint(stage->nir));
    }
 
+   NIR_PASS(_, stage->nir, nir_lower_subdword_loads,
+            (nir_lower_subdword_options) {
+               .modes_1_comp = nir_var_mem_ubo,
+               .modes_N_comps = nir_var_mem_ubo | nir_var_mem_ssbo
+            });
+
    NIR_PASS(_, stage->nir, radv_nir_lower_ycbcr_textures, pipeline_layout);
 
    if (stage->nir->info.uses_resource_info_query)
diff --git a/src/amd/vulkan/radv_pipeline_rt.c b/src/amd/vulkan/radv_pipeline_rt.c
index 49ef06fa36e..9ef71643624 100644
--- a/src/amd/vulkan/radv_pipeline_rt.c
+++ b/src/amd/vulkan/radv_pipeline_rt.c
@@ -901,7 +901,7 @@ parse_rt_stage(struct radv_device *device, const VkPipelineShaderStageCreateInfo
 
    NIR_PASS(_, shader, lower_rt_derefs);
 
-   NIR_PASS(_, shader, nir_lower_explicit_io, nir_var_function_temp,
+   NIR_PASS(_, shader, nir_lower_explicit_io, nir_var_function_temp, false,
             nir_address_format_32bit_offset);
 
    return shader;
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 59be0a8d3d5..3cfc7bc27cf 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -983,7 +983,7 @@ radv_shader_spirv_to_nir(struct radv_device *device, const struct radv_pipeline_
          if (!nir->info.shared_memory_explicit_layout)
             NIR_PASS(_, nir, nir_lower_vars_to_explicit_types, nir_var_mem_shared, shared_var_info);
 
-         NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_shared,
+         NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_shared, false,
                   nir_address_format_32bit_offset);
       }
 
@@ -1068,9 +1068,9 @@ radv_shader_spirv_to_nir(struct radv_device *device, const struct radv_pipeline_
    };
    NIR_PASS(_, nir, nir_opt_access, &opt_access_options);
 
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_push_const, nir_address_format_32bit_offset);
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_push_const, false, nir_address_format_32bit_offset);
 
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ubo | nir_var_mem_ssbo,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ubo | nir_var_mem_ssbo, false,
             nir_address_format_vec2_index_32bit_offset);
 
    NIR_PASS(_, nir, lower_intrinsics, key);
@@ -1088,7 +1088,7 @@ radv_shader_spirv_to_nir(struct radv_device *device, const struct radv_pipeline_
       if (!nir->info.shared_memory_explicit_layout) {
          NIR_PASS(_, nir, nir_lower_vars_to_explicit_types, var_modes, shared_var_info);
       }
-      NIR_PASS(_, nir, nir_lower_explicit_io, var_modes, nir_address_format_32bit_offset);
+      NIR_PASS(_, nir, nir_lower_explicit_io, var_modes, false, nir_address_format_32bit_offset);
 
       if (nir->info.zero_initialize_shared_memory && nir->info.shared_size > 0) {
          const unsigned chunk_size = 16; /* max single store size */
@@ -1097,7 +1097,7 @@ radv_shader_spirv_to_nir(struct radv_device *device, const struct radv_pipeline_
       }
    }
 
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_global | nir_var_mem_constant,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_global | nir_var_mem_constant, false,
             nir_address_format_64bit_global);
 
    /* Lower large variables that are always constant with load_constant
diff --git a/src/compiler/glsl/gl_nir_lower_buffers.c b/src/compiler/glsl/gl_nir_lower_buffers.c
index 12a2aebb858..0dfb4085b59 100644
--- a/src/compiler/glsl/gl_nir_lower_buffers.c
+++ b/src/compiler/glsl/gl_nir_lower_buffers.c
@@ -353,7 +353,7 @@ gl_nir_lower_buffers(nir_shader *shader,
     */
    if (progress) {
       nir_validate_shader(shader, "Lowering buffer interface derefs");
-      nir_lower_explicit_io(shader, nir_var_mem_ubo | nir_var_mem_ssbo,
+      nir_lower_explicit_io(shader, nir_var_mem_ubo | nir_var_mem_ssbo, false,
                             nir_address_format_32bit_index_offset);
    }
 
diff --git a/src/compiler/nir/meson.build b/src/compiler/nir/meson.build
index f1a1879f6d2..837bb972db3 100644
--- a/src/compiler/nir/meson.build
+++ b/src/compiler/nir/meson.build
@@ -199,6 +199,7 @@ files_libnir = files(
   'nir_lower_shader_calls.c',
   'nir_lower_single_sampled.c',
   'nir_lower_ssbo.c',
+  'nir_lower_subdword_loads.c',
   'nir_lower_subgroups.c',
   'nir_lower_system_values.c',
   'nir_lower_task_shader.c',
diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index 18bcfe59dd0..7f17054ba50 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -4875,6 +4875,7 @@ void nir_lower_explicit_io_instr(struct nir_builder *b,
 
 bool nir_lower_explicit_io(nir_shader *shader,
                            nir_variable_mode modes,
+                           bool skip_samplers,
                            nir_address_format);
 
 typedef struct nir_lower_shader_calls_options {
@@ -4956,7 +4957,7 @@ bool nir_lower_phis_to_scalar(nir_shader *shader, bool lower_all);
 void nir_lower_io_arrays_to_elements(nir_shader *producer, nir_shader *consumer);
 void nir_lower_io_arrays_to_elements_no_indirects(nir_shader *shader,
                                                   bool outputs_only);
-void nir_lower_io_to_scalar(nir_shader *shader, nir_variable_mode mask);
+bool nir_lower_io_to_scalar(nir_shader *shader, nir_variable_mode mask);
 bool nir_lower_io_to_scalar_early(nir_shader *shader, nir_variable_mode mask);
 bool nir_lower_io_to_vector(nir_shader *shader, nir_variable_mode mask);
 bool nir_vectorize_tess_levels(nir_shader *shader);
@@ -5795,6 +5796,17 @@ nir_function_impl *nir_shader_get_preamble(nir_shader *shader);
 bool nir_lower_point_smooth(nir_shader *shader);
 bool nir_lower_poly_line_smooth(nir_shader *shader, unsigned num_smooth_aa_sample);
 
+typedef struct {
+   /* Which load instructions to lower depending on whether the number of
+    * components being loaded is 1 or more than 1.
+    */
+   nir_variable_mode modes_1_comp;  /* lower 1-component loads for these */
+   nir_variable_mode modes_N_comps; /* lower multi-component loads for these */
+} nir_lower_subdword_options;
+
+bool nir_lower_subdword_loads(nir_shader *nir,
+                              nir_lower_subdword_options options);
+
 #include "nir_inline_helpers.h"
 
 #ifdef __cplusplus
diff --git a/src/compiler/nir/nir_lower_alu_width.c b/src/compiler/nir/nir_lower_alu_width.c
index c75fc75f574..9b1b3d0a750 100644
--- a/src/compiler/nir/nir_lower_alu_width.c
+++ b/src/compiler/nir/nir_lower_alu_width.c
@@ -328,6 +328,7 @@ lower_alu_instr_width(nir_builder *b, nir_instr *instr, void *_data)
    case nir_op_unpack_64_2x32:
    case nir_op_unpack_64_4x16:
    case nir_op_unpack_32_2x16:
+   case nir_op_unpack_32_4x8:
    case nir_op_unpack_double_2x32_dxil:
       return NULL;
 
diff --git a/src/compiler/nir/nir_lower_io.c b/src/compiler/nir/nir_lower_io.c
index 37bc6a6d377..69902f4deaa 100644
--- a/src/compiler/nir/nir_lower_io.c
+++ b/src/compiler/nir/nir_lower_io.c
@@ -2208,7 +2208,7 @@ lower_explicit_io_mode_check(nir_builder *b, nir_intrinsic_instr *intrin,
 
 static bool
 nir_lower_explicit_io_impl(nir_function_impl *impl, nir_variable_mode modes,
-                           nir_address_format addr_format)
+                           bool skip_samplers, nir_address_format addr_format)
 {
    bool progress = false;
 
@@ -2225,6 +2225,9 @@ nir_lower_explicit_io_impl(nir_function_impl *impl, nir_variable_mode modes,
          case nir_instr_type_deref: {
             nir_deref_instr *deref = nir_instr_as_deref(instr);
             if (nir_deref_mode_is_in_set(deref, modes)) {
+               if (skip_samplers && (
+                  (glsl_type_is_sampler(deref->var->type) || glsl_type_is_texture(deref->var->type))))
+                  break;
                lower_explicit_io_deref(&b, deref, addr_format);
                progress = true;
             }
@@ -2355,13 +2358,13 @@ nir_lower_explicit_io_impl(nir_function_impl *impl, nir_variable_mode modes,
  */
 bool
 nir_lower_explicit_io(nir_shader *shader, nir_variable_mode modes,
-                      nir_address_format addr_format)
+                      bool skip_samplers, nir_address_format addr_format)
 {
    bool progress = false;
 
    nir_foreach_function(function, shader) {
       if (function->impl &&
-          nir_lower_explicit_io_impl(function->impl, modes, addr_format))
+          nir_lower_explicit_io_impl(function->impl, modes, skip_samplers, addr_format))
          progress = true;
    }
 
diff --git a/src/compiler/nir/nir_lower_io_to_scalar.c b/src/compiler/nir/nir_lower_io_to_scalar.c
index ff2c9f07fea..94f4565dbd9 100644
--- a/src/compiler/nir/nir_lower_io_to_scalar.c
+++ b/src/compiler/nir/nir_lower_io_to_scalar.c
@@ -268,14 +268,14 @@ nir_lower_io_to_scalar_instr(nir_builder *b, nir_instr *instr, void *data)
    return false;
 }
 
-void
+bool
 nir_lower_io_to_scalar(nir_shader *shader, nir_variable_mode mask)
 {
-   nir_shader_instructions_pass(shader,
-                                nir_lower_io_to_scalar_instr,
-                                nir_metadata_block_index |
-                                nir_metadata_dominance,
-                                &mask);
+   return nir_shader_instructions_pass(shader,
+                                       nir_lower_io_to_scalar_instr,
+                                       nir_metadata_block_index |
+                                       nir_metadata_dominance,
+                                       &mask);
 }
 
 static nir_variable **
diff --git a/src/compiler/nir/nir_lower_subdword_loads.c b/src/compiler/nir/nir_lower_subdword_loads.c
new file mode 100644
index 00000000000..06466a0b20d
--- /dev/null
+++ b/src/compiler/nir/nir_lower_subdword_loads.c
@@ -0,0 +1,266 @@
+/*
+ * Copyright Â© 2022 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+/* Convert 8-bit and 16-bit UBO loads to 32 bits. This is for drivers that
+ * don't support non-32-bit UBO loads.
+ *
+ * nir_opt_load_store_vectorize should be run before this because it analyzes
+ * offset calculations and recomputes align_mul and align_offset.
+ *
+ * nir_opt_algebraic and (optionally) ALU scalarization are recommended to be
+ * run after this.
+ *
+ * Running nir_opt_load_store_vectorize after this pass may lead to further
+ * vectorization, e.g. adjacent 2x16-bit and 1x32-bit loads will become
+ * 2x32-bit loads.
+ */
+
+#include "nir_builder.h"
+#include "util/u_math.h"
+
+/* Bitcast a vector to a smaller bit size and return a subset of that vector. */
+static nir_ssa_def *
+bitcast_extract_vector(nir_builder *b, nir_ssa_def *src,
+                       unsigned dst_bit_size, unsigned dst_first_component,
+                       unsigned dst_num_components)
+{
+   assert(src->bit_size % dst_bit_size == 0);
+   unsigned dst_elems_per_src_elems = src->bit_size / dst_bit_size;
+   unsigned first_src_elem = dst_first_component / dst_elems_per_src_elems;
+   unsigned end_src_elem =
+      DIV_ROUND_UP(dst_first_component + dst_num_components,
+                   dst_elems_per_src_elems);
+
+   nir_ssa_def **elems = alloca(sizeof(*elems) * end_src_elem *
+                                dst_elems_per_src_elems);
+
+   /* nir_unpack_* is scalar, so we need to apply it to each component. */
+   for (unsigned i = first_src_elem; i < end_src_elem; i++) {
+      nir_ssa_def *vec;
+
+      if (src->bit_size == 32) {
+         if (dst_bit_size == 16)
+            vec = nir_unpack_32_2x16(b, nir_channel(b, src, i));
+         else if (dst_bit_size == 8)
+            vec = nir_unpack_32_4x8(b, nir_channel(b, src, i));
+         else
+            unreachable("unexpected dst_bit_size");
+      } else {
+         unreachable("unexpected src->bit_size");
+      }
+
+      for (unsigned c = 0; c < dst_elems_per_src_elems; c++)
+         elems[i * dst_elems_per_src_elems + c] = nir_channel(b, vec, c);
+   }
+
+   return nir_vec(b, elems + dst_first_component, dst_num_components);
+}
+
+static bool
+lower_subdword_loads(nir_builder *b, nir_instr *instr, void *data)
+{
+   nir_lower_subdword_options *options = data;
+
+   if (instr->type != nir_instr_type_intrinsic)
+      return false;
+
+   nir_intrinsic_instr *intr = nir_instr_as_intrinsic(instr);
+   unsigned num_components = intr->num_components;
+   nir_variable_mode modes =
+      num_components == 1 ? options->modes_1_comp
+                          : options->modes_N_comps;
+
+   switch (intr->intrinsic) {
+   case nir_intrinsic_load_ubo:
+      if (!(modes & nir_var_mem_ubo))
+         return false;
+      break;
+   case nir_intrinsic_load_ssbo:
+      if (!(modes & nir_var_mem_ssbo))
+         return false;
+      break;
+   case nir_intrinsic_load_global:
+      if (!(modes & nir_var_mem_global))
+         return false;
+      break;
+   default:
+      return false;
+   }
+
+   unsigned bit_size = intr->dest.ssa.bit_size;
+   if (bit_size != 8 && bit_size != 16)
+      return false;
+
+   unsigned component_size = bit_size / 8;
+   unsigned comp_per_dword = 4 / component_size;
+
+   /* Get the offset alignment relative to the closest dword. */
+   unsigned align_mul = MIN2(nir_intrinsic_align_mul(intr), 4);
+   unsigned align_offset = nir_intrinsic_align_offset(intr) % align_mul;
+
+   nir_src *src_offset = nir_get_io_offset_src(intr);
+   nir_ssa_def *offset = src_offset->ssa;
+   nir_ssa_def *result = &intr->dest.ssa;
+
+   /* Change the load to 32 bits per channel, update the channel count,
+    * and increase the declared load alignment.
+    */
+   intr->dest.ssa.bit_size = 32;
+
+   if (align_mul == 4 && align_offset == 0) {
+      intr->num_components = intr->dest.ssa.num_components =
+         DIV_ROUND_UP(num_components, comp_per_dword);
+   } else {
+      /* Multi-component unaligned loads may straddle the dword boundary.
+       * E.g. for 2 components, we need to load an extra dword, and so on.
+       */
+      intr->num_components = intr->dest.ssa.num_components =
+         DIV_ROUND_UP(intr->dest.ssa.num_components + comp_per_dword - 1,
+                      comp_per_dword);
+
+      nir_intrinsic_set_align(intr,
+                              MAX2(nir_intrinsic_align_mul(intr), 4),
+                              nir_intrinsic_align_offset(intr) & ~0x3);
+   }
+
+   if (align_mul == 4 && align_offset == 0) {
+      /* Aligned loads. Just bitcast the vector and trim it if there are
+       * trailing unused elements.
+       */
+      b->cursor = nir_after_instr(instr);
+      result = bitcast_extract_vector(b, result, bit_size, 0, num_components);
+
+      nir_ssa_def_rewrite_uses_after(&intr->dest.ssa, result,
+                                     result->parent_instr);
+      return true;
+   }
+
+   if (align_mul == 4) {
+      /* Unaligned loads with an aligned non-constant base offset (which is
+       * X * align_mul) and a constant added offset (align_offset).
+       *
+       * The only difference compared to aligned loads is that we may
+       * overfetch by up to 1 dword and then trim the vector from both sides
+       * (if needed) instead of just the end.
+       */
+      assert(align_offset <= 3);
+      assert(align_offset % component_size == 0);
+      unsigned comp_offset = align_offset / component_size;
+
+      /* There is a good probability that the offset is "iadd" adding
+       * align_offset. Subtracting align_offset should eliminate it.
+       */
+      b->cursor = nir_before_instr(instr);
+      nir_instr_rewrite_src_ssa(instr, src_offset,
+                                nir_iadd_imm(b, offset, -align_offset));
+
+      b->cursor = nir_after_instr(instr);
+      result = bitcast_extract_vector(b, result, bit_size, comp_offset,
+                                      num_components);
+
+      nir_ssa_def_rewrite_uses_after(&intr->dest.ssa, result,
+                                     result->parent_instr);
+      return true;
+   }
+
+   /* Fully unaligned loads. We overfetch up to 1 dword and then bitshift
+    * the whole vector.
+    */
+   assert(align_mul <= 2 && align_offset <= 3);
+
+   /* Round down by masking out the bits. */
+   b->cursor = nir_before_instr(instr);
+   nir_instr_rewrite_src_ssa(instr, src_offset,
+                             nir_iand_imm(b, offset, ~0x3));
+
+   /* We need to shift bits in the loaded vector by this number. */
+   b->cursor = nir_after_instr(instr);
+   nir_ssa_def *shift = nir_ishl_imm(b, nir_iand_imm(b, offset, 0x3), 3);
+
+   /* The following algorithms are used to shift the vector.
+    *
+    * 32-bit variant (shr32 + shl32 + or32 per element):
+    *    for (i = 0; i < n - 1; i++)
+    *       dst[i] = (src[i] >> shift) | (src[i + 1] << (32 - shift));
+    *
+    * 64-bit variant (shr64 + shl32 + or32 per 2 elements):
+    *    for (i = 0; i < n / 2 - 1; i++) {
+    *       qword1 = pack(src[i * 2 + 0], src[i * 2 + 1]) >> shift;
+    *       dword2 = src[i * 2 + 2] << (32 - shift);
+    *       dst[i * 2 + 0] = unpack_64_2x32_x(qword1);
+    *       dst[i * 2 + 1] = unpack_64_2x32_y(qword1) | dword2;
+    *    }
+    */
+   nir_ssa_def **elems =
+      alloca(sizeof(*elems) * intr->num_components);
+
+   nir_ssa_def *rev_shift32 = nir_isub_imm(b, 32, shift);
+
+   unsigned i = 0;
+
+   if (intr->num_components >= 2) {
+      /* Use the 64-bit algorithm first because it's faster. */
+      for (i = 0; i < intr->num_components / 2 - 1; i++) {
+         nir_ssa_def *qword1, *dword2;
+
+         qword1 = nir_pack_64_2x32_split(b,
+                                         nir_channel(b, result, i * 2 + 0),
+                                         nir_channel(b, result, i * 2 + 1));
+         qword1 = nir_ushr(b, qword1, shift);
+         dword2 = nir_ishl(b, nir_channel(b, result, i * 2 + 2),
+                           rev_shift32);
+
+         elems[i * 2 + 0] = nir_unpack_64_2x32_split_x(b, qword1);
+         elems[i * 2 + 1] =
+            nir_ior(b, nir_unpack_64_2x32_split_y(b, qword1), dword2);
+      }
+      i *= 2;
+
+      /* Use the 32-bit algorithm for the remainder of the vector. */
+      for (; i < intr->num_components - 1; i++) {
+         elems[i] = nir_ior(b,
+                        nir_ushr(b, nir_channel(b, result, i), shift),
+                        nir_ishl(b, nir_channel(b, result, i + 1),
+                                 rev_shift32));
+      }
+   }
+
+   /* Shift the last element. */
+   elems[i] = nir_ushr(b, nir_channel(b, result, i), shift);
+
+   result = nir_vec(b, elems, intr->num_components);
+   result = bitcast_extract_vector(b, result, bit_size, 0,
+                                   num_components);
+
+   nir_ssa_def_rewrite_uses_after(&intr->dest.ssa, result,
+                                  result->parent_instr);
+   return true;
+}
+
+bool
+nir_lower_subdword_loads(nir_shader *nir, nir_lower_subdword_options options)
+{
+   return nir_shader_instructions_pass(nir, lower_subdword_loads,
+                                       nir_metadata_dominance |
+                                       nir_metadata_block_index, &options);
+}
diff --git a/src/compiler/nir/nir_opt_algebraic.py b/src/compiler/nir/nir_opt_algebraic.py
index f35232a66ca..d7acdca2617 100644
--- a/src/compiler/nir/nir_opt_algebraic.py
+++ b/src/compiler/nir/nir_opt_algebraic.py
@@ -1059,7 +1059,7 @@ for s in [8, 16, 32, 64]:
        (('ineg', ('b2i{}'.format(s), 'a@{}'.format(s))), a),
 
        # SM5 32-bit shifts are defined to use the 5 least significant bits (or 4 bits for 16 bits)
-       (('ishl', 'a@{}'.format(s), ('iand', s - 1, b)), ('ishl', a, b)),
+#       (('ishl', 'a@{}'.format(s), ('iand', s - 1, b)), ('ishl', a, b)),
        (('ishr', 'a@{}'.format(s), ('iand', s - 1, b)), ('ishr', a, b)),
        (('ushr', 'a@{}'.format(s), ('iand', s - 1, b)), ('ushr', a, b)),
        (('ushr', 'a@{}'.format(s), ('ishl(is_used_once)', ('iand', b, 1), last_shift_bit)), ('ushr', a, ('ishl', b, last_shift_bit))),
@@ -2622,6 +2622,11 @@ late_optimizations = [
    (('~feq', ('fadd(is_used_once)', a, b), 0.0), ('feq', a, ('fneg', b))),
    (('~fneu', ('fadd(is_used_once)', a, b), 0.0), ('fneu', a, ('fneg', b))),
 
+   (('imul_high@8' , a, b), ('i2i8',  ('ishr', ('imul', ('i2i32', a), ('i2i32', b)),  8))),
+   (('umul_high@8' , a, b), ('u2u8',  ('ushr', ('imul', ('u2u32', a), ('u2u32', b)),  8))),
+   (('imul_high@16', a, b), ('i2i16', ('ishr', ('imul', ('i2i32', a), ('i2i32', b)), 16))),
+   (('umul_high@16', a, b), ('u2u16', ('ushr', ('imul', ('u2u32', a), ('u2u32', b)), 16))),
+
    # If either source must be finite, then the original (a+b) cannot produce
    # NaN due to Inf-Inf.  The patterns and the replacements produce the same
    # result if b is NaN. Therefore, the replacements are exact.
diff --git a/src/compiler/spirv/vtn_opencl.c b/src/compiler/spirv/vtn_opencl.c
index c41628fd475..5c2ec15a15f 100644
--- a/src/compiler/spirv/vtn_opencl.c
+++ b/src/compiler/spirv/vtn_opencl.c
@@ -297,7 +297,6 @@ static const struct {
    REMAP(Normalize, "normalize"),
    REMAP(Degrees, "degrees"),
    REMAP(Radians, "radians"),
-   REMAP(Rotate, "rotate"),
    REMAP(Smoothstep, "smoothstep"),
    REMAP(Step, "step"),
 
@@ -553,6 +552,8 @@ handle_special(struct vtn_builder *b, uint32_t opcode,
       if (nb->shader->options->lower_ffma32 && srcs[0]->bit_size == 32)
          break;
       return nir_ffma(nb, srcs[0], srcs[1], srcs[2]);
+   case OpenCLstd_Rotate:
+         return nir_urol(nb, srcs[0], nir_iand(nb, nir_imm_int(nb, srcs[0]->bit_size - 1), nir_u2u(nb, srcs[1], 32)));
    default:
       break;
    }
diff --git a/src/gallium/drivers/radeonsi/si_get.c b/src/gallium/drivers/radeonsi/si_get.c
index 2cd374f39d4..8c918667ecd 100644
--- a/src/gallium/drivers/radeonsi/si_get.c
+++ b/src/gallium/drivers/radeonsi/si_get.c
@@ -1177,7 +1177,7 @@ void si_init_screen_get_functions(struct si_screen *sscreen)
       .lower_fisnormal = true,
       .lower_rotate = true,
       .lower_to_scalar = true,
-      .lower_int64_options = nir_lower_imul_2x32_64,
+      .lower_int64_options = nir_lower_imul_2x32_64 | nir_lower_imul_high64,
       .has_sdot_4x8 = sscreen->info.has_accelerated_dot_product,
       .has_sudot_4x8 = sscreen->info.has_accelerated_dot_product && sscreen->info.gfx_level >= GFX11,
       .has_udot_4x8 = sscreen->info.has_accelerated_dot_product,
diff --git a/src/gallium/drivers/radeonsi/si_shader.c b/src/gallium/drivers/radeonsi/si_shader.c
index feda45dade7..b04dd4f4030 100644
--- a/src/gallium/drivers/radeonsi/si_shader.c
+++ b/src/gallium/drivers/radeonsi/si_shader.c
@@ -2134,7 +2134,7 @@ si_get_shader_part(struct si_screen *sscreen, struct si_shader_part **list,
    }
 
    struct si_shader_context ctx;
-   si_llvm_context_init(&ctx, sscreen, compiler, wave32 ? 32 : 64);
+   si_llvm_context_init(&ctx, sscreen, compiler, wave32 ? 32 : 64, stage);
 
    ctx.shader = &shader;
    ctx.stage = stage;
diff --git a/src/gallium/drivers/radeonsi/si_shader_internal.h b/src/gallium/drivers/radeonsi/si_shader_internal.h
index 3ce257881b6..88d5a1fb042 100644
--- a/src/gallium/drivers/radeonsi/si_shader_internal.h
+++ b/src/gallium/drivers/radeonsi/si_shader_internal.h
@@ -200,7 +200,8 @@ bool si_compile_llvm(struct si_screen *sscreen, struct si_shader_binary *binary,
                      struct ac_llvm_context *ac, struct util_debug_callback *debug,
                      gl_shader_stage stage, const char *name, bool less_optimized);
 void si_llvm_context_init(struct si_shader_context *ctx, struct si_screen *sscreen,
-                          struct ac_llvm_compiler *compiler, unsigned wave_size);
+                          struct ac_llvm_compiler *compiler, unsigned wave_size,
+                          gl_shader_stage stage);
 void si_llvm_create_func(struct si_shader_context *ctx, const char *name, LLVMTypeRef *return_types,
                          unsigned num_return_elems, unsigned max_workgroup_size);
 void si_llvm_create_main_func(struct si_shader_context *ctx, bool ngg_cull_shader);
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index 075f8d13dbf..2d066e7c3c2 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -127,14 +127,16 @@ bool si_compile_llvm(struct si_screen *sscreen, struct si_shader_binary *binary,
 }
 
 void si_llvm_context_init(struct si_shader_context *ctx, struct si_screen *sscreen,
-                          struct ac_llvm_compiler *compiler, unsigned wave_size)
+                          struct ac_llvm_compiler *compiler, unsigned wave_size,
+                          gl_shader_stage stage)
 {
+   enum ac_float_mode float_mode = stage == MESA_SHADER_KERNEL ? AC_FLOAT_MODE_DEFAULT : AC_FLOAT_MODE_DEFAULT_OPENGL;
    memset(ctx, 0, sizeof(*ctx));
    ctx->screen = sscreen;
    ctx->compiler = compiler;
 
    ac_llvm_context_init(&ctx->ac, compiler, sscreen->info.gfx_level, sscreen->info.family,
-                        sscreen->info.has_3d_cube_border_color_mipmap, AC_FLOAT_MODE_DEFAULT_OPENGL, wave_size, 64);
+                        sscreen->info.has_3d_cube_border_color_mipmap, float_mode, wave_size, 64);
 }
 
 void si_llvm_create_func(struct si_shader_context *ctx, const char *name, LLVMTypeRef *return_types,
@@ -1304,7 +1306,7 @@ bool si_llvm_compile_shader(struct si_screen *sscreen, struct ac_llvm_compiler *
    struct si_shader_selector *sel = shader->selector;
    struct si_shader_context ctx;
 
-   si_llvm_context_init(&ctx, sscreen, compiler, shader->wave_size);
+   si_llvm_context_init(&ctx, sscreen, compiler, shader->wave_size, nir->info.stage);
    ctx.so = *so;
 
    struct ac_llvm_pointer ngg_cull_main_fn = {};
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c b/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
index 363267119bc..886c71b5f41 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm_gs.c
@@ -485,7 +485,7 @@ struct si_shader *si_generate_gs_copy_shader(struct si_screen *sscreen,
       shader->info.vs_output_param_mask |= BITFIELD64_BIT(i);
    }
 
-   si_llvm_context_init(&ctx, sscreen, compiler, shader->wave_size);
+   si_llvm_context_init(&ctx, sscreen, compiler, shader->wave_size, MESA_SHADER_VERTEX);
    ctx.shader = shader;
    ctx.stage = MESA_SHADER_VERTEX;
    ctx.so = *so;
diff --git a/src/gallium/drivers/radeonsi/si_shader_nir.c b/src/gallium/drivers/radeonsi/si_shader_nir.c
index 6fac0ae9f5d..52c69154993 100644
--- a/src/gallium/drivers/radeonsi/si_shader_nir.c
+++ b/src/gallium/drivers/radeonsi/si_shader_nir.c
@@ -342,7 +342,12 @@ char *si_finalize_nir(struct pipe_screen *screen, void *nirptr)
 
    nir_lower_io_passes(nir);
 
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_shared, nir_address_format_32bit_offset);
+   NIR_PASS_V(nir, nir_lower_subdword_loads,
+              (nir_lower_subdword_options) {
+                 .modes_1_comp = nir_var_mem_ubo,
+                 .modes_N_comps = nir_var_mem_ubo | nir_var_mem_ssbo
+              });
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_shared, false, nir_address_format_32bit_offset);
 
    /* Remove dead derefs, so that we can remove uniforms. */
    NIR_PASS_V(nir, nir_opt_dce);
diff --git a/src/gallium/frontends/clover/nir/invocation.cpp b/src/gallium/frontends/clover/nir/invocation.cpp
index d75d79a8c72..5fc80b9dbd1 100644
--- a/src/gallium/frontends/clover/nir/invocation.cpp
+++ b/src/gallium/frontends/clover/nir/invocation.cpp
@@ -359,7 +359,7 @@ binary clover::nir::spirv_to_nir(const binary &mod, const device &dev,
                                              nir->constant_data_size,
                                              nir_var_mem_constant);
       }
-      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_constant,
+      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_constant, false,
                  spirv_options.constant_addr_format);
 
       auto args = sym.args;
@@ -380,20 +380,20 @@ binary clover::nir::spirv_to_nir(const binary &mod, const device &dev,
       NIR_PASS_V(nir, nir_lower_memcpy);
 
       /* use offsets for kernel inputs (uniform) */
-      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_uniform,
+      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_uniform, false,
                  nir->info.cs.ptr_size == 64 ?
                  nir_address_format_32bit_offset_as_64bit :
                  nir_address_format_32bit_offset);
 
-      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_constant,
+      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_constant, false,
                  spirv_options.constant_addr_format);
-      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_shared,
+      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_shared, false,
                  spirv_options.shared_addr_format);
 
-      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_function_temp,
+      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_function_temp, false,
                  spirv_options.temp_addr_format);
 
-      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_global,
+      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_global, false,
                  spirv_options.global_addr_format);
 
       struct nir_remove_dead_variables_options remove_dead_variables_options = {};
diff --git a/src/gallium/frontends/lavapipe/lvp_pipeline.c b/src/gallium/frontends/lavapipe/lvp_pipeline.c
index 2efd36c9c45..22cad864c76 100644
--- a/src/gallium/frontends/lavapipe/lvp_pipeline.c
+++ b/src/gallium/frontends/lavapipe/lvp_pipeline.c
@@ -460,20 +460,20 @@ lvp_shader_compile_to_ir(struct lvp_pipeline *pipeline,
    NIR_PASS_V(nir, nir_split_var_copies);
    NIR_PASS_V(nir, nir_lower_global_vars_to_local);
 
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_push_const,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_push_const, false,
               nir_address_format_32bit_offset);
 
    NIR_PASS_V(nir, nir_lower_explicit_io,
-              nir_var_mem_ubo | nir_var_mem_ssbo,
+              nir_var_mem_ubo | nir_var_mem_ssbo, false,
               nir_address_format_32bit_index_offset);
 
    NIR_PASS_V(nir, nir_lower_explicit_io,
-              nir_var_mem_global,
+              nir_var_mem_global, false,
               nir_address_format_64bit_global);
 
    if (nir->info.stage == MESA_SHADER_COMPUTE) {
       NIR_PASS_V(nir, nir_lower_vars_to_explicit_types, nir_var_mem_shared, shared_var_info);
-      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_shared, nir_address_format_32bit_offset);
+      NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_shared, false, nir_address_format_32bit_offset);
    }
 
    NIR_PASS_V(nir, nir_remove_dead_variables, nir_var_shader_temp, NULL);
diff --git a/src/gallium/frontends/rusticl/core/device.rs b/src/gallium/frontends/rusticl/core/device.rs
index 5a73f6f389c..e34589ba681 100644
--- a/src/gallium/frontends/rusticl/core/device.rs
+++ b/src/gallium/frontends/rusticl/core/device.rs
@@ -536,8 +536,8 @@ impl Device {
     }
 
     pub fn const_max_size(&self) -> cl_ulong {
-        self.screen
-            .param(pipe_cap::PIPE_CAP_MAX_SHADER_BUFFER_SIZE_UINT) as u64
+        // for now return mem alloc size as we don't use proper UBOs yet
+        self.max_mem_alloc()
     }
 
     pub fn device_type(&self, internal: bool) -> cl_device_type {
@@ -705,10 +705,7 @@ impl Device {
     }
 
     pub fn param_max_size(&self) -> usize {
-        ComputeParam::<u64>::compute_param(
-            self.screen.as_ref(),
-            pipe_compute_cap::PIPE_COMPUTE_CAP_MAX_INPUT_SIZE,
-        ) as usize
+        self.shader_param(pipe_shader_cap::PIPE_SHADER_CAP_MAX_CONST_BUFFER0_SIZE) as usize
     }
 
     pub fn printf_buffer_size(&self) -> usize {
diff --git a/src/gallium/frontends/rusticl/core/kernel.rs b/src/gallium/frontends/rusticl/core/kernel.rs
index 2d35682fc43..3e892197b8d 100644
--- a/src/gallium/frontends/rusticl/core/kernel.rs
+++ b/src/gallium/frontends/rusticl/core/kernel.rs
@@ -657,18 +657,20 @@ fn lower_and_optimize_nir_late(
         shared_address_format = nir_address_format::nir_address_format_32bit_offset_as_64bit;
     }
 
-    nir.pass2(
+    nir.pass3(
         nir_lower_explicit_io,
         nir_variable_mode::nir_var_mem_global | nir_variable_mode::nir_var_mem_constant,
+        false,
         global_address_format,
     );
 
     nir.pass1(rusticl_lower_intrinsics, &mut lower_state);
-    nir.pass2(
+    nir.pass3(
         nir_lower_explicit_io,
         nir_variable_mode::nir_var_mem_shared
             | nir_variable_mode::nir_var_function_temp
             | nir_variable_mode::nir_var_uniform,
+        dev.samplers_as_deref(),
         shared_address_format,
     );
 
@@ -817,7 +819,7 @@ fn extract<'a, const S: usize>(buf: &'a mut &[u8]) -> &'a [u8; S] {
 }
 
 fn optimize_local_size(d: &Device, grid: &mut [u32; 3], block: &mut [u32; 3]) {
-    let mut threads = d.max_threads_per_block() as u32;
+    let mut threads = d.max_threads_per_block() as u32 / 4;
     let dim_threads = d.max_block_sizes();
     let subgroups = d.subgroups();
 
@@ -1017,7 +1019,7 @@ impl Kernel {
                     let buf = Arc::new(
                         q.device
                             .screen
-                            .resource_create_buffer(printf_size, ResourceType::Normal)
+                            .resource_create_buffer(printf_size, ResourceType::Staging)
                             .unwrap(),
                     );
 
diff --git a/src/gallium/frontends/rusticl/mesa/pipe/screen.rs b/src/gallium/frontends/rusticl/mesa/pipe/screen.rs
index 37bd1b88dbb..c8b9a19f18f 100644
--- a/src/gallium/frontends/rusticl/mesa/pipe/screen.rs
+++ b/src/gallium/frontends/rusticl/mesa/pipe/screen.rs
@@ -93,7 +93,7 @@ impl PipeScreen {
                 (*self.screen).context_create.unwrap()(
                     self.screen,
                     ptr::null_mut(),
-                    PIPE_CONTEXT_COMPUTE_ONLY,
+                    0, //PIPE_CONTEXT_COMPUTE_ONLY,
                 )
             },
             self,
diff --git a/src/gallium/targets/rusticl/meson.build b/src/gallium/targets/rusticl/meson.build
index a968dee52db..c3a9cf27eb0 100644
--- a/src/gallium/targets/rusticl/meson.build
+++ b/src/gallium/targets/rusticl/meson.build
@@ -45,6 +45,7 @@ librusticl = shared_library(
     driver_iris,
     driver_nouveau,
     driver_panfrost,
+    driver_radeonsi,
     driver_swrast,
     idep_nir,
   ],
diff --git a/src/imagination/rogue/rogue_nir.c b/src/imagination/rogue/rogue_nir.c
index 9c6be45cc54..49194d5f42f 100644
--- a/src/imagination/rogue/rogue_nir.c
+++ b/src/imagination/rogue/rogue_nir.c
@@ -130,7 +130,7 @@ bool rogue_nir_passes(struct rogue_build_ctx *ctx,
    /* Additional I/O lowering. */
    NIR_PASS_V(nir,
               nir_lower_explicit_io,
-              nir_var_mem_ubo,
+              nir_var_mem_ubo, false,
               spirv_options.ubo_addr_format);
    NIR_PASS_V(nir, rogue_nir_lower_io, NULL);
 
diff --git a/src/intel/compiler/brw_kernel.c b/src/intel/compiler/brw_kernel.c
index fd5f4e62180..53a1e64e6c5 100644
--- a/src/intel/compiler/brw_kernel.c
+++ b/src/intel/compiler/brw_kernel.c
@@ -427,15 +427,15 @@ brw_kernel_from_spirv(struct brw_compiler *compiler,
 
    NIR_PASS_V(nir, nir_lower_memcpy);
 
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_constant,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_constant, false,
               nir_address_format_64bit_global);
 
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_uniform,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_uniform, false,
               nir_address_format_32bit_offset_as_64bit);
 
    NIR_PASS_V(nir, nir_lower_explicit_io,
               nir_var_shader_temp | nir_var_function_temp |
-              nir_var_mem_shared | nir_var_mem_global,
+              nir_var_mem_shared | nir_var_mem_global, false,
               nir_address_format_62bit_generic);
 
    NIR_PASS_V(nir, nir_lower_frexp);
diff --git a/src/intel/compiler/brw_mesh.cpp b/src/intel/compiler/brw_mesh.cpp
index 2250771f365..be0e68a360b 100644
--- a/src/intel/compiler/brw_mesh.cpp
+++ b/src/intel/compiler/brw_mesh.cpp
@@ -165,7 +165,7 @@ brw_nir_lower_tue_outputs(nir_shader *nir, brw_tue_map *map)
    NIR_PASS(_, nir, nir_lower_vars_to_explicit_types,
             nir_var_mem_task_payload, shared_type_info);
    NIR_PASS(_, nir, nir_lower_explicit_io,
-            nir_var_mem_task_payload, nir_address_format_32bit_offset);
+            nir_var_mem_task_payload, false, nir_address_format_32bit_offset);
 
    map->size_dw = ALIGN(DIV_ROUND_UP(nir->info.task_payload_size, 4), 8);
 }
@@ -364,7 +364,7 @@ brw_nir_lower_tue_inputs(nir_shader *nir, const brw_tue_map *map)
       nir->info.task_payload_size = 0;
    }
 
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_task_payload,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_task_payload, false,
             nir_address_format_32bit_offset);
 }
 
diff --git a/src/intel/compiler/brw_nir.c b/src/intel/compiler/brw_nir.c
index 1ab004b68d4..f094c595332 100644
--- a/src/intel/compiler/brw_nir.c
+++ b/src/intel/compiler/brw_nir.c
@@ -1271,7 +1271,7 @@ brw_postprocess_nir(nir_shader *nir, const struct brw_compiler *compiler,
    if (is_scalar && nir_shader_has_local_variables(nir)) {
       OPT(nir_lower_vars_to_explicit_types, nir_var_function_temp,
           glsl_get_natural_size_align_bytes);
-      OPT(nir_lower_explicit_io, nir_var_function_temp,
+      OPT(nir_lower_explicit_io, nir_var_function_temp, false,
           nir_address_format_32bit_offset);
       brw_nir_optimize(nir, compiler, is_scalar, false);
    }
diff --git a/src/intel/compiler/brw_nir_rt.c b/src/intel/compiler/brw_nir_rt.c
index b124928b293..b18248391ab 100644
--- a/src/intel/compiler/brw_nir_rt.c
+++ b/src/intel/compiler/brw_nir_rt.c
@@ -218,7 +218,7 @@ lower_rt_io_and_scratch(nir_shader *nir)
    NIR_PASS_V(nir, nir_lower_explicit_io,
               nir_var_function_temp |
               nir_var_mem_constant |
-              nir_var_ray_hit_attrib,
+              nir_var_ray_hit_attrib, false,
               nir_address_format_64bit_global);
 }
 
diff --git a/src/intel/vulkan/anv_pipeline.c b/src/intel/vulkan/anv_pipeline.c
index 72806ca62e9..d2197f665e8 100644
--- a/src/intel/vulkan/anv_pipeline.c
+++ b/src/intel/vulkan/anv_pipeline.c
@@ -918,9 +918,9 @@ anv_pipeline_lower_nir(struct anv_pipeline *pipeline,
 
    NIR_PASS(_, nir, brw_nir_lower_storage_image, compiler->devinfo);
 
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_global,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_global, false,
             nir_address_format_64bit_global);
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_push_const,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_push_const, false,
             nir_address_format_32bit_offset);
 
    NIR_PASS(_, nir, brw_nir_lower_ray_queries, &pdevice->info);
@@ -933,10 +933,10 @@ anv_pipeline_lower_nir(struct anv_pipeline *pipeline,
               pdevice, pipeline->device->robust_buffer_access,
               layout, &stage->bind_map);
 
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ubo,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ubo, false,
             anv_nir_ubo_addr_format(pdevice,
                pipeline->device->robust_buffer_access));
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ssbo,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ssbo, false,
             anv_nir_ssbo_addr_format(pdevice,
                pipeline->device->robust_buffer_access));
 
@@ -981,7 +981,7 @@ anv_pipeline_lower_nir(struct anv_pipeline *pipeline,
       }
 
       NIR_PASS(_, nir, nir_lower_explicit_io,
-               nir_var_mem_shared, nir_address_format_32bit_offset);
+               nir_var_mem_shared, false, nir_address_format_32bit_offset);
 
       if (nir->info.zero_initialize_shared_memory &&
           nir->info.shared_size > 0) {
diff --git a/src/intel/vulkan/anv_pipeline_cache.c b/src/intel/vulkan/anv_pipeline_cache.c
index e3894df6a42..88e163ae538 100644
--- a/src/intel/vulkan/anv_pipeline_cache.c
+++ b/src/intel/vulkan/anv_pipeline_cache.c
@@ -473,7 +473,7 @@ anv_load_fp64_shader(struct anv_device *device)
    NIR_PASS_V(nir, nir_opt_peephole_select, 1, false, false);
    NIR_PASS_V(nir, nir_opt_dce);
 
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_function_temp,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_function_temp, false,
               nir_address_format_62bit_generic);
 
    anv_device_upload_nir(device, device->internal_cache,
diff --git a/src/intel/vulkan_hasvk/anv_pipeline.c b/src/intel/vulkan_hasvk/anv_pipeline.c
index a0be151af6b..1fdd3bb6e1c 100644
--- a/src/intel/vulkan_hasvk/anv_pipeline.c
+++ b/src/intel/vulkan_hasvk/anv_pipeline.c
@@ -642,9 +642,9 @@ anv_pipeline_lower_nir(struct anv_pipeline *pipeline,
 
    NIR_PASS(_, nir, brw_nir_lower_storage_image, compiler->devinfo);
 
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_global,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_global, false,
             nir_address_format_64bit_global);
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_push_const,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_push_const, false,
             nir_address_format_32bit_offset);
 
    NIR_PASS(_, nir, brw_nir_lower_ray_queries, &pdevice->info);
@@ -654,10 +654,10 @@ anv_pipeline_lower_nir(struct anv_pipeline *pipeline,
               pdevice, pipeline->device->robust_buffer_access,
               layout, &stage->bind_map);
 
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ubo,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ubo, false,
             anv_nir_ubo_addr_format(pdevice,
                pipeline->device->robust_buffer_access));
-   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ssbo,
+   NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ssbo, false,
             anv_nir_ssbo_addr_format(pdevice,
                pipeline->device->robust_buffer_access));
 
@@ -702,7 +702,7 @@ anv_pipeline_lower_nir(struct anv_pipeline *pipeline,
       }
 
       NIR_PASS(_, nir, nir_lower_explicit_io,
-               nir_var_mem_shared, nir_address_format_32bit_offset);
+               nir_var_mem_shared, false, nir_address_format_32bit_offset);
 
       if (nir->info.zero_initialize_shared_memory &&
           nir->info.shared_size > 0) {
diff --git a/src/mesa/state_tracker/st_glsl_to_nir.cpp b/src/mesa/state_tracker/st_glsl_to_nir.cpp
index a994e65f81b..ee61446bc30 100644
--- a/src/mesa/state_tracker/st_glsl_to_nir.cpp
+++ b/src/mesa/state_tracker/st_glsl_to_nir.cpp
@@ -419,7 +419,7 @@ st_nir_preprocess(struct st_context *st, struct gl_program *prog,
       NIR_PASS_V(prog->nir, nir_lower_vars_to_explicit_types,
                  nir_var_mem_shared, shared_type_info);
       NIR_PASS_V(prog->nir, nir_lower_explicit_io,
-                 nir_var_mem_shared, nir_address_format_32bit_offset);
+                 nir_var_mem_shared, false, nir_address_format_32bit_offset);
    }
 
    /* Do a round of constant folding to clean up address calculations */
diff --git a/src/microsoft/clc/clc_compiler.c b/src/microsoft/clc/clc_compiler.c
index 92a97a42e7f..b97019cedf3 100644
--- a/src/microsoft/clc/clc_compiler.c
+++ b/src/microsoft/clc/clc_compiler.c
@@ -1020,10 +1020,10 @@ clc_spirv_to_dxil(struct clc_libclc *lib,
    NIR_PASS_V(nir, split_unaligned_loads_stores);
 
    assert(nir->info.cs.ptr_size == 64);
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ssbo,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ssbo, false,
               nir_address_format_32bit_index_offset_pack64);
    NIR_PASS_V(nir, nir_lower_explicit_io,
-              nir_var_mem_shared | nir_var_function_temp | nir_var_uniform,
+              nir_var_mem_shared | nir_var_function_temp | nir_var_uniform, false,
               nir_address_format_32bit_offset_as_64bit);
 
    NIR_PASS_V(nir, nir_lower_system_values);
@@ -1078,7 +1078,7 @@ clc_spirv_to_dxil(struct clc_libclc *lib,
 
    NIR_PASS_V(nir, clc_nir_lower_kernel_input_loads, inputs_var);
    NIR_PASS_V(nir, split_unaligned_loads_stores);
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ubo,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ubo, false,
               nir_address_format_32bit_index_offset);
    NIR_PASS_V(nir, clc_nir_lower_system_values, work_properties_var);
    NIR_PASS_V(nir, dxil_nir_lower_loads_stores_to_dxil);
diff --git a/src/microsoft/spirv_to_dxil/dxil_spirv_nir.c b/src/microsoft/spirv_to_dxil/dxil_spirv_nir.c
index dc15f4e2024..834febea0fe 100644
--- a/src/microsoft/spirv_to_dxil/dxil_spirv_nir.c
+++ b/src/microsoft/spirv_to_dxil/dxil_spirv_nir.c
@@ -652,7 +652,7 @@ dxil_spirv_nir_passes(nir_shader *nir,
               NULL);
 
    uint32_t push_constant_size = 0;
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_push_const,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_push_const, false,
               nir_address_format_32bit_offset);
    NIR_PASS_V(nir, dxil_spirv_nir_lower_load_push_constant,
               nir_address_format_32bit_index_offset,
@@ -660,14 +660,14 @@ dxil_spirv_nir_passes(nir_shader *nir,
               conf->push_constant_cbv.base_shader_register,
               &push_constant_size);
 
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ubo | nir_var_mem_ssbo,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ubo | nir_var_mem_ssbo, false,
               nir_address_format_32bit_index_offset);
 
    if (!nir->info.shared_memory_explicit_layout) {
       NIR_PASS_V(nir, nir_lower_vars_to_explicit_types, nir_var_mem_shared,
                  shared_var_info);
    }
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_shared,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_shared, false,
       nir_address_format_32bit_offset_as_64bit);
 
    NIR_PASS_V(nir, nir_lower_clip_cull_distance_arrays);
diff --git a/src/nouveau/codegen/nv50_ir_from_nir.cpp b/src/nouveau/codegen/nv50_ir_from_nir.cpp
index d31dc13d11c..54290a6abaa 100644
--- a/src/nouveau/codegen/nv50_ir_from_nir.cpp
+++ b/src/nouveau/codegen/nv50_ir_from_nir.cpp
@@ -3343,7 +3343,7 @@ Converter::run()
 
    /* codegen assumes vec4 alignment for memory */
    NIR_PASS_V(nir, nir_lower_vars_to_explicit_types, nir_var_function_temp, function_temp_type_info);
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_function_temp, nir_address_format_32bit_offset);
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_function_temp, false, nir_address_format_32bit_offset);
    NIR_PASS_V(nir, nir_remove_dead_variables, nir_var_function_temp, NULL);
 
    NIR_PASS_V(nir, nir_lower_io, nir_var_shader_in | nir_var_shader_out,
diff --git a/src/panfrost/vulkan/panvk_vX_shader.c b/src/panfrost/vulkan/panvk_vX_shader.c
index fde8445ca98..049d1e7e9c9 100644
--- a/src/panfrost/vulkan/panvk_vX_shader.c
+++ b/src/panfrost/vulkan/panvk_vX_shader.c
@@ -316,12 +316,12 @@ panvk_per_arch(shader_create)(struct panvk_device *dev,
    NIR_PASS_V(nir, panvk_per_arch(nir_lower_descriptors),
               dev, layout, &shader->has_img_access);
 
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ubo,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ubo, false,
               nir_address_format_32bit_index_offset);
-   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ssbo,
+   NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_ssbo, false,
               spirv_options.ssbo_addr_format);
    NIR_PASS_V(nir, nir_lower_explicit_io,
-              nir_var_mem_push_const,
+              nir_var_mem_push_const, false,
               nir_address_format_32bit_offset);
 
    if (gl_shader_stage_uses_workgroup(stage)) {
@@ -332,7 +332,7 @@ panvk_per_arch(shader_create)(struct panvk_device *dev,
       }
 
       NIR_PASS_V(nir, nir_lower_explicit_io,
-                 nir_var_mem_shared,
+                 nir_var_mem_shared, false,
                  nir_address_format_32bit_offset);
    }
 
