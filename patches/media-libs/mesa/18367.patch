From be4cd226e6ef0ac4ed6751707b1fe667affdcaca Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Mon, 28 Feb 2022 09:15:44 +0100
Subject: [PATCH 1/5] ac/nir/ngg: Add EXT_mesh_shader primitive indices.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

In EXT_mesh_shader the indices output is an array of vectors
which is indexed by the primitive index.

(They practically behave like a per-primitive output, although
technically the spec does not treat them as per-primitive.)

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
Reviewed-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/common/ac_nir_lower_ngg.c | 22 +++++++++++++++++++++-
 1 file changed, 21 insertions(+), 1 deletion(-)

diff --git a/src/amd/common/ac_nir_lower_ngg.c b/src/amd/common/ac_nir_lower_ngg.c
index 4615ab971c36..52e2992a23c9 100644
--- a/src/amd/common/ac_nir_lower_ngg.c
+++ b/src/amd/common/ac_nir_lower_ngg.c
@@ -2635,8 +2635,28 @@ ms_store_arrayed_output_intrin(nir_builder *b,
                                nir_intrinsic_instr *intrin,
                                lower_ngg_ms_state *s)
 {
-   ms_out_mode out_mode;
    unsigned location = nir_intrinsic_io_semantics(intrin).location;
+
+   if (location == VARYING_SLOT_PRIMITIVE_INDICES) {
+      /* EXT_mesh_shader primitive indices: array of vectors.
+       * They don't count as per-primitive outputs, but the array is indexed
+       * by the primitive index, so they are practically per-primitive.
+       *
+       * The max vertex count is 256, so these indices always fit 8 bits.
+       * To reduce LDS use, store these as a flat array of 8-bit values.
+       */
+      assert(nir_src_is_const(*nir_get_io_offset_src(intrin)));
+      assert(nir_src_as_uint(*nir_get_io_offset_src(intrin)) == 0);
+      assert(nir_intrinsic_component(intrin) == 0);
+
+      nir_ssa_def *store_val = intrin->src[0].ssa;
+      nir_ssa_def *arr_index = nir_get_io_arrayed_index_src(intrin)->ssa;
+      nir_ssa_def *offset = nir_imul_imm(b, arr_index, s->vertices_per_prim);
+      ms_store_prim_indices(b, store_val, offset, s);
+      return;
+   }
+
+   ms_out_mode out_mode;
    const ms_out_part *out = ms_get_out_layout_part(location, &b->shader->info, &out_mode, s);
    update_ms_output_info(intrin, out, s);
 
-- 
GitLab


From f8445bacaab47778fafc136b526c19d3b8a96b36 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Mon, 28 Feb 2022 20:12:00 +0100
Subject: [PATCH 2/5] ac/nir/ngg: Add EXT_mesh_shader CullPrimitiveEXT output.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

This is a per-primitive boolean output.
When set to 1, the primitive should be culled.

Implement this by using this boolean as the null primitive
flag for primitive exports.

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
Reviewed-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/common/ac_nir_lower_ngg.c | 58 ++++++++++++++++++++++++++++---
 1 file changed, 54 insertions(+), 4 deletions(-)

diff --git a/src/amd/common/ac_nir_lower_ngg.c b/src/amd/common/ac_nir_lower_ngg.c
index 52e2992a23c9..5b9b95255a9d 100644
--- a/src/amd/common/ac_nir_lower_ngg.c
+++ b/src/amd/common/ac_nir_lower_ngg.c
@@ -144,6 +144,7 @@ typedef struct
       ms_out_part vtx_attr;
       ms_out_part prm_attr;
       uint32_t indices_addr;
+      uint32_t cull_flags_addr;
       uint32_t total_size;
    } lds;
    /* VRAM "mesh shader scratch ring" layout for outputs that don't fit into the LDS. */
@@ -174,6 +175,8 @@ typedef struct
 
    /* True if the lowering needs to insert the layer output. */
    bool insert_layer_output;
+   /* True if cull flags are used */
+   bool uses_cull_flags;
 
    struct {
       /* Bitmask of components used: 4 bits per slot, 1 bit per component. */
@@ -2439,6 +2442,21 @@ ms_load_num_prims(nir_builder *b,
    return nir_load_shared(b, 1, 32, addr, .base = s->layout.lds.workgroup_info_addr + lds_ms_num_prims);
 }
 
+static void
+ms_store_cull_flag(nir_builder *b,
+                   nir_ssa_def *val,
+                   nir_ssa_def *offset_src,
+                   lower_ngg_ms_state *s)
+{
+   assert(val->num_components == 1);
+   assert(val->bit_size == 1);
+
+   if (!offset_src)
+      offset_src = nir_imm_int(b, 0);
+
+   nir_store_shared(b, nir_b2i8(b, val), offset_src, .base = s->layout.lds.cull_flags_addr);
+}
+
 static nir_ssa_def *
 lower_ms_store_output(nir_builder *b,
                       nir_intrinsic_instr *intrin,
@@ -2654,6 +2672,20 @@ ms_store_arrayed_output_intrin(nir_builder *b,
       nir_ssa_def *offset = nir_imul_imm(b, arr_index, s->vertices_per_prim);
       ms_store_prim_indices(b, store_val, offset, s);
       return;
+   } else if (location == VARYING_SLOT_CULL_PRIMITIVE) {
+      /* EXT_mesh_shader cull primitive: per-primitive bool.
+       * To reduce LDS use, store these as an array of 8-bit values.
+       */
+      assert(nir_src_is_const(*nir_get_io_offset_src(intrin)));
+      assert(nir_src_as_uint(*nir_get_io_offset_src(intrin)) == 0);
+      assert(nir_intrinsic_component(intrin) == 0);
+      assert(nir_intrinsic_write_mask(intrin) == 1);
+
+      nir_ssa_def *store_val = intrin->src[0].ssa;
+      nir_ssa_def *arr_index = nir_get_io_arrayed_index_src(intrin)->ssa;
+      nir_ssa_def *offset = nir_imul_imm(b, arr_index, s->vertices_per_prim);
+      ms_store_cull_flag(b, store_val, offset, s);
+      return;
    }
 
    ms_out_mode out_mode;
@@ -3054,6 +3086,13 @@ emit_ms_finale(nir_builder *b, lower_ngg_ms_state *s)
       /* Primitive connectivity data: describes which vertices the primitive uses. */
       nir_ssa_def *prim_idx_addr = nir_imul_imm(b, invocation_index, s->vertices_per_prim);
       nir_ssa_def *indices_loaded = nir_load_shared(b, s->vertices_per_prim, 8, prim_idx_addr, .base = s->layout.lds.indices_addr);
+      nir_ssa_def *cull_flag = NULL;
+
+      if (s->uses_cull_flags) {
+         nir_ssa_def *loaded_cull_flag = nir_load_shared(b, 1, 8, prim_idx_addr, .base = s->layout.lds.cull_flags_addr);
+         cull_flag = nir_i2b1(b, nir_u2u32(b, loaded_cull_flag));
+      }
+
       nir_ssa_def *indices[3];
       nir_ssa_def *max_vtx_idx = nir_iadd_imm(b, num_vtx, -1u);
 
@@ -3062,7 +3101,7 @@ emit_ms_finale(nir_builder *b, lower_ngg_ms_state *s)
          indices[i] = nir_umin(b, indices[i], max_vtx_idx);
       }
 
-      nir_ssa_def *prim_exp_arg = emit_pack_ngg_prim_exp_arg(b, s->vertices_per_prim, indices, NULL, false);
+      nir_ssa_def *prim_exp_arg = emit_pack_ngg_prim_exp_arg(b, s->vertices_per_prim, indices, cull_flag, false);
       nir_export_primitive_amd(b, prim_exp_arg);
    }
    nir_pop_if(b, if_has_output_primitive);
@@ -3241,7 +3280,8 @@ ms_calculate_output_layout(unsigned api_shared_size,
                            uint64_t cross_invocation_output_access,
                            unsigned max_vertices,
                            unsigned max_primitives,
-                           unsigned vertices_per_prim)
+                           unsigned vertices_per_prim,
+                           bool uses_cull)
 {
    uint64_t lds_per_vertex_output_mask = per_vertex_output_mask & cross_invocation_output_access;
    uint64_t lds_per_primitive_output_mask = per_primitive_output_mask & cross_invocation_output_access;
@@ -3290,6 +3330,12 @@ ms_calculate_output_layout(unsigned api_shared_size,
    l.lds.indices_addr = ALIGN(l.lds.total_size, 16);
    l.lds.total_size = l.lds.indices_addr + max_primitives * vertices_per_prim;
 
+   if (uses_cull) {
+      /* Cull flags: array of 8-bit cull flags for each primitive, 1=cull, 0=keep. */
+      l.lds.cull_flags_addr = ALIGN(l.lds.total_size, 16);
+      l.lds.total_size = l.lds.cull_flags_addr + max_primitives;
+   }
+
    /* NGG is only allowed to address up to 32K of LDS. */
    assert(l.lds.total_size <= 32 * 1024);
    return l;
@@ -3305,12 +3351,15 @@ ac_nir_lower_ngg_ms(nir_shader *shader,
       num_mesh_vertices_per_primitive(shader->info.mesh.primitive_type);
 
    uint64_t special_outputs =
-      BITFIELD64_BIT(VARYING_SLOT_PRIMITIVE_COUNT) | BITFIELD64_BIT(VARYING_SLOT_PRIMITIVE_INDICES);
+      BITFIELD64_BIT(VARYING_SLOT_PRIMITIVE_COUNT) | BITFIELD64_BIT(VARYING_SLOT_PRIMITIVE_INDICES) |
+      BITFIELD64_BIT(VARYING_SLOT_CULL_PRIMITIVE);
    uint64_t per_vertex_outputs =
       shader->info.outputs_written & ~shader->info.per_primitive_outputs & ~special_outputs;
    uint64_t per_primitive_outputs =
       shader->info.per_primitive_outputs & shader->info.outputs_written & ~special_outputs;
 
+   /* Whether the shader uses CullPrimitiveEXT */
+   bool uses_cull = shader->info.outputs_written & BITFIELD64_BIT(VARYING_SLOT_CULL_PRIMITIVE);
    /* Can't handle indirect register addressing, pretend as if they were cross-invocation. */
    uint64_t cross_invocation_access = shader->info.mesh.ms_cross_invocation_output_access |
                                       shader->info.outputs_accessed_indirectly;
@@ -3320,7 +3369,7 @@ ac_nir_lower_ngg_ms(nir_shader *shader,
 
    ms_out_mem_layout layout =
       ms_calculate_output_layout(shader->info.shared_size, per_vertex_outputs, per_primitive_outputs,
-                                 cross_invocation_access, max_vertices, max_primitives, vertices_per_prim);
+                                 cross_invocation_access, max_vertices, max_primitives, vertices_per_prim, uses_cull);
 
    shader->info.shared_size = layout.lds.total_size;
    *out_needs_scratch_ring = layout.vram.vtx_attr.mask || layout.vram.prm_attr.mask;
@@ -3348,6 +3397,7 @@ ac_nir_lower_ngg_ms(nir_shader *shader,
       .api_workgroup_size = api_workgroup_size,
       .hw_workgroup_size = hw_workgroup_size,
       .insert_layer_output = multiview && !(shader->info.outputs_written & VARYING_BIT_LAYER),
+      .uses_cull_flags = uses_cull,
    };
 
    nir_function_impl *impl = nir_shader_get_entrypoint(shader);
-- 
GitLab


From 92e0eb4bfb16fd194105053501a5d0fe7490e73f Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Mon, 28 Feb 2022 14:24:17 +0100
Subject: [PATCH 3/5] ac/nir/ngg: Add EXT_mesh_shader vertex/primitive count.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

In EXT_mesh_shader the vertex and primitive counts are set using a
built-in SetMeshOutputsEXT function.

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
Reviewed-by: Rhys Perry <pendingchaos02@gmail.com>
---
 src/amd/common/ac_nir_lower_ngg.c | 115 +++++++++++++++++++++++++++++-
 1 file changed, 113 insertions(+), 2 deletions(-)

diff --git a/src/amd/common/ac_nir_lower_ngg.c b/src/amd/common/ac_nir_lower_ngg.c
index 5b9b95255a9d..24a125c7c15f 100644
--- a/src/amd/common/ac_nir_lower_ngg.c
+++ b/src/amd/common/ac_nir_lower_ngg.c
@@ -172,6 +172,8 @@ typedef struct
 
    nir_ssa_def *workgroup_index;
    nir_variable *out_variables[VARYING_SLOT_MAX * 4];
+   nir_variable *primitive_count_var;
+   nir_variable *vertex_count_var;
 
    /* True if the lowering needs to insert the layer output. */
    bool insert_layer_output;
@@ -2816,6 +2818,25 @@ lower_ms_load_workgroup_index(nir_builder *b,
    return s->workgroup_index;
 }
 
+static nir_ssa_def *
+lower_ms_set_vertex_and_primitive_count(nir_builder *b,
+                                        nir_intrinsic_instr *intrin,
+                                        lower_ngg_ms_state *s)
+{
+   /* If either the number of vertices or primitives is zero, set both of them to zero. */
+   nir_ssa_def *num_vtx = nir_read_first_invocation(b, intrin->src[0].ssa);
+   nir_ssa_def *num_prm = nir_read_first_invocation(b, intrin->src[1].ssa);
+   nir_ssa_def *zero = nir_imm_int(b, 0);
+   nir_ssa_def *is_either_zero = nir_ieq(b, nir_umin(b, num_vtx, num_prm), zero);
+   num_vtx = nir_bcsel(b, is_either_zero, zero, num_vtx);
+   num_prm = nir_bcsel(b, is_either_zero, zero, num_prm);
+
+   nir_store_var(b, s->vertex_count_var, num_vtx, 0x1);
+   nir_store_var(b, s->primitive_count_var, num_prm, 0x1);
+
+   return NIR_LOWER_INSTR_PROGRESS_REPLACE;
+}
+
 static nir_ssa_def *
 update_ms_scoped_barrier(nir_builder *b,
                          nir_intrinsic_instr *intrin,
@@ -2861,6 +2882,8 @@ lower_ms_intrinsic(nir_builder *b, nir_instr *instr, void *state)
       return update_ms_scoped_barrier(b, intrin, s);
    case nir_intrinsic_load_workgroup_index:
       return lower_ms_load_workgroup_index(b, intrin, s);
+   case nir_intrinsic_set_vertex_and_primitive_count:
+      return lower_ms_set_vertex_and_primitive_count(b, intrin, s);
    default:
       unreachable("Not a lowerable mesh shader intrinsic.");
    }
@@ -2881,7 +2904,8 @@ filter_ms_intrinsic(const nir_instr *instr,
           intrin->intrinsic == nir_intrinsic_store_per_primitive_output ||
           intrin->intrinsic == nir_intrinsic_load_per_primitive_output ||
           intrin->intrinsic == nir_intrinsic_scoped_barrier ||
-          intrin->intrinsic == nir_intrinsic_load_workgroup_index;
+          intrin->intrinsic == nir_intrinsic_load_workgroup_index ||
+          intrin->intrinsic == nir_intrinsic_set_vertex_and_primitive_count;
 }
 
 static void
@@ -3032,6 +3056,85 @@ set_nv_ms_final_output_counts(nir_builder *b,
    *out_num_vtx = num_vtx;
 }
 
+static void
+set_ms_final_output_counts(nir_builder *b,
+                           lower_ngg_ms_state *s,
+                           nir_ssa_def **out_num_prm,
+                           nir_ssa_def **out_num_vtx)
+{
+   /* The spec allows the numbers to be divergent, and in that case we need to
+    * use the values from the first invocation. Also the HW requires us to set
+    * both to 0 if either was 0.
+    *
+    * These are already done by the lowering.
+    */
+   nir_ssa_def *num_prm = nir_load_var(b, s->primitive_count_var);
+   nir_ssa_def *num_vtx = nir_load_var(b, s->vertex_count_var);
+
+   if (s->hw_workgroup_size <= s->wave_size) {
+      /* Single-wave mesh shader workgroup. */
+      nir_alloc_vertices_and_primitives_amd(b, num_vtx, num_prm);
+      *out_num_prm = num_prm;
+      *out_num_vtx = num_vtx;
+      return;
+   }
+
+   /* Multi-wave mesh shader workgroup:
+    * We need to use LDS to distribute the correct values to the other waves.
+    *
+    * TODO:
+    * If we can prove that the values are workgroup-uniform, we can skip this
+    * and just use whatever the current wave has. However, NIR divergence analysis
+    * currently doesn't support this.
+    */
+
+   nir_ssa_def *zero = nir_imm_int(b, 0);
+
+   nir_if *if_wave_0 = nir_push_if(b, nir_ieq_imm(b, nir_load_subgroup_id(b), 0));
+   {
+      nir_if *if_elected = nir_push_if(b, nir_elect(b, 1));
+      {
+         nir_store_shared(b, nir_vec2(b, num_prm, num_vtx), zero,
+                          .base = s->layout.lds.workgroup_info_addr + lds_ms_num_prims);
+      }
+      nir_pop_if(b, if_elected);
+
+      nir_scoped_barrier(b, .execution_scope = NIR_SCOPE_WORKGROUP,
+                            .memory_scope = NIR_SCOPE_WORKGROUP,
+                            .memory_semantics = NIR_MEMORY_ACQ_REL,
+                            .memory_modes = nir_var_mem_shared);
+
+      nir_alloc_vertices_and_primitives_amd(b, num_vtx, num_prm);
+   }
+   nir_push_else(b, if_wave_0);
+   {
+      nir_scoped_barrier(b, .execution_scope = NIR_SCOPE_WORKGROUP,
+                            .memory_scope = NIR_SCOPE_WORKGROUP,
+                            .memory_semantics = NIR_MEMORY_ACQ_REL,
+                            .memory_modes = nir_var_mem_shared);
+
+      nir_ssa_def *prm_vtx = NULL;
+      nir_ssa_def *dont_care_2x32 = nir_ssa_undef(b, 2, 32);
+      nir_if *if_elected = nir_push_if(b, nir_elect(b, 1));
+      {
+         prm_vtx = nir_load_shared(b, 2, 32, zero,
+                                   .base = s->layout.lds.workgroup_info_addr + lds_ms_num_prims);
+      }
+      nir_pop_if(b, if_elected);
+
+      prm_vtx = nir_if_phi(b, prm_vtx, dont_care_2x32);
+      num_prm = nir_read_first_invocation(b, nir_channel(b, prm_vtx, 0));
+      num_vtx = nir_read_first_invocation(b, nir_channel(b, prm_vtx, 1));
+
+      nir_store_var(b, s->primitive_count_var, num_prm, 0x1);
+      nir_store_var(b, s->vertex_count_var, num_vtx, 0x1);
+   }
+   nir_pop_if(b, if_wave_0);
+
+   *out_num_prm = nir_load_var(b, s->primitive_count_var);
+   *out_num_vtx = nir_load_var(b, s->vertex_count_var);
+}
+
 static void
 emit_ms_finale(nir_builder *b, lower_ngg_ms_state *s)
 {
@@ -3045,7 +3148,10 @@ emit_ms_finale(nir_builder *b, lower_ngg_ms_state *s)
    nir_ssa_def *num_prm;
    nir_ssa_def *num_vtx;
 
-   set_nv_ms_final_output_counts(b, s, &num_prm, &num_vtx);
+   if (b->shader->info.mesh.nv)
+      set_nv_ms_final_output_counts(b, s, &num_prm, &num_vtx);
+   else
+      set_ms_final_output_counts(b, s, &num_prm, &num_vtx);
 
    nir_ssa_def *invocation_index = nir_load_local_invocation_index(b);
 
@@ -3403,6 +3509,11 @@ ac_nir_lower_ngg_ms(nir_shader *shader,
    nir_function_impl *impl = nir_shader_get_entrypoint(shader);
    assert(impl);
 
+   state.vertex_count_var =
+      nir_local_variable_create(impl, glsl_uint_type(), "vertex_count_var");
+   state.primitive_count_var =
+      nir_local_variable_create(impl, glsl_uint_type(), "primitive_count_var");
+
    nir_builder builder;
    nir_builder *b = &builder; /* This is to avoid the & */
    nir_builder_init(b, impl);
-- 
GitLab


From 71c906dd98d7a3e7ba41b6a89c1fb0fa5c72966e Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Mon, 28 Feb 2022 14:25:19 +0100
Subject: [PATCH 4/5] radv: Implement EXT_mesh_shader draw calls.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

There is a new packet DISPATCH_MESH_INDIRECT_MULTI which we use
for indirect mesh shader only draws. This packet allows using
a 3D dispatch (however firstTask doesn't work with this packet).

Otherwise everything else works the same way as
the NV_mesh_shader draw calls.

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
Reviewed-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
Reviewed-by: Bas Nieuwenhuizen <bas@basnieuwenhuizen.nl>
---
 src/amd/vulkan/radv_cmd_buffer.c | 169 +++++++++++++++++++++++++++++++
 1 file changed, 169 insertions(+)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index f1a22a032595..92e06d2a97fb 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -6383,6 +6383,40 @@ radv_cs_emit_indirect_draw_packet(struct radv_cmd_buffer *cmd_buffer, bool index
    }
 }
 
+ALWAYS_INLINE static void
+radv_cs_emit_indirect_mesh_draw_packet(struct radv_cmd_buffer *cmd_buffer, uint32_t draw_count,
+                                       uint64_t count_va, uint32_t stride)
+{
+   struct radeon_cmdbuf *cs = cmd_buffer->cs;
+   bool draw_id_enable = cmd_buffer->state.graphics_pipeline->uses_drawid;
+   uint32_t base_reg = cmd_buffer->state.graphics_pipeline->vtx_base_sgpr;
+   bool predicating = cmd_buffer->state.predicating;
+   assert(base_reg);
+
+   /* Reset draw state. */
+   cmd_buffer->state.last_first_instance = -1;
+   cmd_buffer->state.last_num_instances = -1;
+   cmd_buffer->state.last_drawid = -1;
+   cmd_buffer->state.last_vertex_offset = -1;
+
+   /* Note: firstTask/firstVertex is not supported by this draw packet. */
+   uint32_t xyz_dim_reg = (base_reg + 4 - SI_SH_REG_OFFSET) >> 2;
+   uint32_t draw_id_reg = (base_reg + 16 - SI_SH_REG_OFFSET) >> 2;
+
+   radeon_emit(cs, PKT3(PKT3_DISPATCH_MESH_INDIRECT_MULTI, 7, predicating));
+   radeon_emit(cs, 0); /* data_offset */
+   radeon_emit(cs, (xyz_dim_reg & 0xFFFF) | (draw_id_reg << 16));
+   radeon_emit(cs,
+               S_2C3_DRAW_INDEX_ENABLE(draw_id_enable) | S_2C3_COUNT_INDIRECT_ENABLE(!!count_va));
+   radeon_emit(cs, draw_count);
+   radeon_emit(cs, count_va & 0xFFFFFFFF);
+   radeon_emit(cs, count_va >> 32);
+   radeon_emit(cs, stride);
+   radeon_emit(cs, V_0287F0_DI_SRC_SEL_AUTO_INDEX);
+
+   cmd_buffer->state.uses_draw_indirect_multi = true;
+}
+
 ALWAYS_INLINE static void
 radv_cs_emit_dispatch_taskmesh_direct_ace_packet(struct radv_cmd_buffer *cmd_buffer,
                                                  const uint32_t x, const uint32_t y,
@@ -6824,6 +6858,43 @@ radv_emit_direct_mesh_draw_packet(struct radv_cmd_buffer *cmd_buffer,
    }
 }
 
+ALWAYS_INLINE static inline void
+radv_emit_indirect_mesh_draw_packets(struct radv_cmd_buffer *cmd_buffer,
+                                     const struct radv_draw_info *info)
+{
+   const struct radv_cmd_state *state = &cmd_buffer->state;
+   struct radeon_winsys *ws = cmd_buffer->device->ws;
+   struct radeon_cmdbuf *cs = cmd_buffer->cs;
+   const uint64_t va =
+      radv_buffer_get_va(info->indirect->bo) + info->indirect->offset + info->indirect_offset;
+   const uint64_t count_va = !info->count_buffer
+                                ? 0
+                                : radv_buffer_get_va(info->count_buffer->bo) +
+                                     info->count_buffer->offset + info->count_buffer_offset;
+
+   radv_cs_add_buffer(ws, cs, info->indirect->bo);
+
+   if (info->count_buffer) {
+      radv_cs_add_buffer(ws, cs, info->count_buffer->bo);
+   }
+
+   radeon_emit(cs, PKT3(PKT3_SET_BASE, 2, 0));
+   radeon_emit(cs, 1);
+   radeon_emit(cs, va);
+   radeon_emit(cs, va >> 32);
+
+   radv_emit_userdata_mesh_first_task_0_draw_id_0(cmd_buffer);
+
+   if (!state->subpass->view_mask) {
+      radv_cs_emit_indirect_mesh_draw_packet(cmd_buffer, info->count, count_va, info->stride);
+   } else {
+      u_foreach_bit (i, state->subpass->view_mask) {
+         radv_emit_view_index(cmd_buffer, i);
+         radv_cs_emit_indirect_mesh_draw_packet(cmd_buffer, info->count, count_va, info->stride);
+      }
+   }
+}
+
 ALWAYS_INLINE static void
 radv_emit_direct_taskmesh_draw_packets(struct radv_cmd_buffer *cmd_buffer, uint32_t x, uint32_t y,
                                        uint32_t z, uint32_t first_task)
@@ -7843,6 +7914,34 @@ radv_CmdDrawMeshTasksNV(VkCommandBuffer commandBuffer, uint32_t taskCount, uint3
    radv_after_draw(cmd_buffer);
 }
 
+VKAPI_ATTR void VKAPI_CALL
+radv_CmdDrawMeshTasksEXT(VkCommandBuffer commandBuffer, uint32_t x, uint32_t y, uint32_t z)
+{
+   RADV_FROM_HANDLE(radv_cmd_buffer, cmd_buffer, commandBuffer);
+   struct radv_graphics_pipeline *pipeline = cmd_buffer->state.graphics_pipeline;
+   struct radv_draw_info info;
+
+   info.count = x * y * z;
+   info.instance_count = 1;
+   info.first_instance = 0;
+   info.stride = 0;
+   info.indexed = false;
+   info.strmout_buffer = NULL;
+   info.count_buffer = NULL;
+   info.indirect = NULL;
+
+   if (!radv_before_taskmesh_draw(cmd_buffer, &info, 1))
+      return;
+
+   if (radv_pipeline_has_stage(pipeline, MESA_SHADER_TASK)) {
+      radv_emit_direct_taskmesh_draw_packets(cmd_buffer, x, y, z, 0);
+   } else {
+      radv_emit_direct_mesh_draw_packet(cmd_buffer, x, y, z, 0);
+   }
+
+   radv_after_draw(cmd_buffer);
+}
+
 VKAPI_ATTR void VKAPI_CALL
 radv_CmdDrawMeshTasksIndirectNV(VkCommandBuffer commandBuffer, VkBuffer _buffer,
                                 VkDeviceSize offset, uint32_t drawCount, uint32_t stride)
@@ -7902,6 +8001,40 @@ radv_CmdDrawMeshTasksIndirectNV(VkCommandBuffer commandBuffer, VkBuffer _buffer,
    radv_after_draw(cmd_buffer);
 }
 
+VKAPI_ATTR void VKAPI_CALL
+radv_CmdDrawMeshTasksIndirectEXT(VkCommandBuffer commandBuffer, VkBuffer _buffer,
+                                 VkDeviceSize offset, uint32_t drawCount, uint32_t stride)
+{
+   if (!drawCount)
+      return;
+
+   RADV_FROM_HANDLE(radv_cmd_buffer, cmd_buffer, commandBuffer);
+   RADV_FROM_HANDLE(radv_buffer, buffer, _buffer);
+
+   struct radv_graphics_pipeline *pipeline = cmd_buffer->state.graphics_pipeline;
+   struct radv_draw_info info;
+
+   info.indirect = buffer;
+   info.indirect_offset = offset;
+   info.stride = stride;
+   info.count = drawCount;
+   info.strmout_buffer = NULL;
+   info.count_buffer = NULL;
+   info.indexed = false;
+   info.instance_count = 0;
+
+   if (!radv_before_taskmesh_draw(cmd_buffer, &info, drawCount))
+      return;
+
+   if (radv_pipeline_has_stage(pipeline, MESA_SHADER_TASK)) {
+      radv_emit_indirect_taskmesh_draw_packets(cmd_buffer, &info, 0, 0);
+   } else {
+      radv_emit_indirect_mesh_draw_packets(cmd_buffer, &info);
+   }
+
+   radv_after_draw(cmd_buffer);
+}
+
 VKAPI_ATTR void VKAPI_CALL
 radv_CmdDrawMeshTasksIndirectCountNV(VkCommandBuffer commandBuffer, VkBuffer _buffer,
                                      VkDeviceSize offset, VkBuffer _countBuffer,
@@ -7951,6 +8084,42 @@ radv_CmdDrawMeshTasksIndirectCountNV(VkCommandBuffer commandBuffer, VkBuffer _bu
    radv_after_draw(cmd_buffer);
 }
 
+VKAPI_ATTR void VKAPI_CALL
+radv_CmdDrawMeshTasksIndirectCountEXT(VkCommandBuffer commandBuffer, VkBuffer _buffer,
+                                      VkDeviceSize offset, VkBuffer _countBuffer,
+                                      VkDeviceSize countBufferOffset, uint32_t maxDrawCount,
+                                      uint32_t stride)
+{
+
+   RADV_FROM_HANDLE(radv_cmd_buffer, cmd_buffer, commandBuffer);
+   RADV_FROM_HANDLE(radv_buffer, buffer, _buffer);
+   RADV_FROM_HANDLE(radv_buffer, count_buffer, _countBuffer);
+
+   struct radv_graphics_pipeline *pipeline = cmd_buffer->state.graphics_pipeline;
+   struct radv_draw_info info;
+
+   info.indirect = buffer;
+   info.indirect_offset = offset;
+   info.stride = stride;
+   info.count = maxDrawCount;
+   info.strmout_buffer = NULL;
+   info.count_buffer = count_buffer;
+   info.count_buffer_offset = countBufferOffset;
+   info.indexed = false;
+   info.instance_count = 0;
+
+   if (!radv_before_taskmesh_draw(cmd_buffer, &info, maxDrawCount))
+      return;
+
+   if (radv_pipeline_has_stage(pipeline, MESA_SHADER_TASK)) {
+      radv_emit_indirect_taskmesh_draw_packets(cmd_buffer, &info, 0, 0);
+   } else {
+      radv_emit_indirect_mesh_draw_packets(cmd_buffer, &info);
+   }
+
+   radv_after_draw(cmd_buffer);
+}
+
 void
 radv_CmdExecuteGeneratedCommandsNV(VkCommandBuffer commandBuffer, VkBool32 isPreprocessed,
                                    const VkGeneratedCommandsInfoNV *pGeneratedCommandsInfo)
-- 
GitLab


From 69a85b7327f4d562c2e21329915c59c645b336d3 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Timur=20Krist=C3=B3f?= <timur.kristof@gmail.com>
Date: Thu, 20 Jan 2022 16:36:28 +0100
Subject: [PATCH 5/5] radv: Enable EXT_mesh_shader on RDNA2 with
 RADV_PERFTEST=ext_ms
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Timur Kristóf <timur.kristof@gmail.com>
Reviewed-by: Rhys Perry <pendingchaos02@gmail.com>
Reviewed-by: Samuel Pitoiset <samuel.pitoiset@gmail.com>
---
 src/amd/ci/gitlab-ci.yml     |  2 +-
 src/amd/ci/radv-skips.txt    |  2 ++
 src/amd/vulkan/radv_debug.h  |  1 +
 src/amd/vulkan/radv_device.c | 63 ++++++++++++++++++++++++++++++++++++
 src/amd/vulkan/radv_shader.c |  1 +
 5 files changed, 68 insertions(+), 1 deletion(-)

diff --git a/src/amd/ci/gitlab-ci.yml b/src/amd/ci/gitlab-ci.yml
index ad35d5945729..216768f23295 100644
--- a/src/amd/ci/gitlab-ci.yml
+++ b/src/amd/ci/gitlab-ci.yml
@@ -132,7 +132,7 @@ radv_stoney_vkcts:amd64:
     - .deqp-test-valve
   variables:
     DEQP_VER: vk
-    RADV_PERFTEST: nv_ms
+    RADV_PERFTEST: nv_ms,ext_ms
 
 # VKCTS never finishes on gfx7 due to all the GPU resets and hangs.
 # Hence, disable it for now.
diff --git a/src/amd/ci/radv-skips.txt b/src/amd/ci/radv-skips.txt
index 6bd118583352..ecbf954b2efb 100644
--- a/src/amd/ci/radv-skips.txt
+++ b/src/amd/ci/radv-skips.txt
@@ -58,4 +58,6 @@ dEQP-VK.spirv_assembly.instruction.graphics.16bit_storage.uniform_32struct_to_16
 # These tests create an ACE cmdbuf which waits for GFX, thus can cause
 # a deadlock when executed in parallel without gang submit.
 dEQP-VK.mesh_shader.nv.synchronization.*
+dEQP-VK.mesh_shader.ext.synchronization.*
 dEQP-VK.mesh_shader.nv.misc.many_task*
+dEQP-VK.mesh_shader.ext.misc.many_task*
diff --git a/src/amd/vulkan/radv_debug.h b/src/amd/vulkan/radv_debug.h
index 930dbd1c56f2..9b1833ed329b 100644
--- a/src/amd/vulkan/radv_debug.h
+++ b/src/amd/vulkan/radv_debug.h
@@ -84,6 +84,7 @@ enum {
    RADV_PERFTEST_NV_MS = 1u << 11,
    RADV_PERFTEST_RT_WAVE_64 = 1u << 12,
    RADV_PERFTEST_GPL = 1u << 13,
+   RADV_PERFTEST_EXT_MS = 1u << 14,
 };
 
 bool radv_init_trace(struct radv_device *device);
diff --git a/src/amd/vulkan/radv_device.c b/src/amd/vulkan/radv_device.c
index d4c7db6ca1ea..1a15610bdd22 100644
--- a/src/amd/vulkan/radv_device.c
+++ b/src/amd/vulkan/radv_device.c
@@ -548,6 +548,8 @@ radv_physical_device_get_supported_extensions(const struct radv_physical_device
       .EXT_line_rasterization = true,
       .EXT_memory_budget = true,
       .EXT_memory_priority = true,
+      .EXT_mesh_shader =
+         radv_taskmesh_enabled(device) && device->instance->perftest_flags & RADV_PERFTEST_EXT_MS,
       .EXT_multi_draw = true,
       .EXT_non_seamless_cube_map = true,
       .EXT_pci_bus_info = true,
@@ -1025,6 +1027,7 @@ static const struct debug_control radv_perftest_options[] = {{"localbos", RADV_P
                                                              {"nv_ms", RADV_PERFTEST_NV_MS},
                                                              {"rtwave64", RADV_PERFTEST_RT_WAVE_64},
                                                              {"gpl", RADV_PERFTEST_GPL},
+                                                             {"ext_ms", RADV_PERFTEST_EXT_MS},
                                                              {NULL, 0}};
 
 const char *
@@ -1738,6 +1741,17 @@ radv_GetPhysicalDeviceFeatures2(VkPhysicalDevice physicalDevice,
          features->taskShader = features->meshShader = radv_taskmesh_enabled(pdevice);
          break;
       }
+      case VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_MESH_SHADER_FEATURES_EXT: {
+         VkPhysicalDeviceMeshShaderFeaturesEXT *features =
+            (VkPhysicalDeviceMeshShaderFeaturesEXT *)ext;
+         bool taskmesh_en = radv_taskmesh_enabled(pdevice);
+         features->meshShader = taskmesh_en;
+         features->taskShader = taskmesh_en;
+         features->multiviewMeshShader = taskmesh_en;
+         features->primitiveFragmentShadingRateMeshShader = taskmesh_en;
+         features->meshShaderQueries = false;
+         break;
+      }
       case VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_TEXTURE_COMPRESSION_ASTC_HDR_FEATURES: {
          VkPhysicalDeviceTextureCompressionASTCHDRFeatures *features =
             (VkPhysicalDeviceTextureCompressionASTCHDRFeatures *)ext;
@@ -2548,6 +2562,55 @@ radv_GetPhysicalDeviceProperties2(VkPhysicalDevice physicalDevice,
          props->graphicsPipelineLibraryIndependentInterpolationDecoration = false;
          break;
       }
+      case VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_MESH_SHADER_PROPERTIES_EXT: {
+         VkPhysicalDeviceMeshShaderPropertiesEXT *properties =
+            (VkPhysicalDeviceMeshShaderPropertiesEXT *)ext;
+
+         properties->maxTaskWorkGroupTotalCount = 4194304; /* 2^22 min required */
+         properties->maxTaskWorkGroupCount[0] = 65535;
+         properties->maxTaskWorkGroupCount[1] = 65535;
+         properties->maxTaskWorkGroupCount[2] = 65535;
+         properties->maxTaskWorkGroupInvocations = 1024;
+         properties->maxTaskWorkGroupSize[0] = 1024;
+         properties->maxTaskWorkGroupSize[1] = 1024;
+         properties->maxTaskWorkGroupSize[2] = 1024;
+         properties->maxTaskPayloadSize = 16384; /* 16K min required */
+         properties->maxTaskSharedMemorySize = 65536;
+         properties->maxTaskPayloadAndSharedMemorySize = 65536;
+
+         properties->maxMeshWorkGroupTotalCount = 4194304; /* 2^22 min required */
+         properties->maxMeshWorkGroupCount[0] = 65535;
+         properties->maxMeshWorkGroupCount[1] = 65535;
+         properties->maxMeshWorkGroupCount[2] = 65535;
+         properties->maxMeshWorkGroupInvocations = 256; /* Max NGG HW limit */
+         properties->maxMeshWorkGroupSize[0] = 256;
+         properties->maxMeshWorkGroupSize[1] = 256;
+         properties->maxMeshWorkGroupSize[2] = 256;
+         properties->maxMeshOutputMemorySize = 32 * 1024; /* 32K min required */
+         properties->maxMeshSharedMemorySize = 28672;     /* 28K min required */
+         properties->maxMeshPayloadAndSharedMemorySize =
+            properties->maxTaskPayloadSize +
+            properties->maxMeshSharedMemorySize; /* 28K min required */
+         properties->maxMeshPayloadAndOutputMemorySize =
+            properties->maxTaskPayloadSize +
+            properties->maxMeshOutputMemorySize;    /* 47K min required */
+         properties->maxMeshOutputComponents = 128; /* 32x vec4 min required */
+         properties->maxMeshOutputVertices = 256;
+         properties->maxMeshOutputPrimitives = 256;
+         properties->maxMeshOutputLayers = 8;
+         properties->maxMeshMultiviewViewCount = MAX_VIEWS;
+         properties->meshOutputPerVertexGranularity = 1;
+         properties->meshOutputPerPrimitiveGranularity = 1;
+
+         properties->maxPreferredTaskWorkGroupInvocations = 1024;
+         properties->maxPreferredMeshWorkGroupInvocations = 128;
+         properties->prefersLocalInvocationVertexOutput = true;
+         properties->prefersLocalInvocationPrimitiveOutput = true;
+         properties->prefersCompactVertexOutput = true;
+         properties->prefersCompactPrimitiveOutput = false;
+
+         break;
+      }
       default:
          break;
       }
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 6f9b9a47ddb3..e6b783e31a1b 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -740,6 +740,7 @@ radv_shader_spirv_to_nir(struct radv_device *device, const struct radv_pipeline_
                .int64 = true,
                .int64_atomics = true,
                .integer_functions2 = true,
+               .mesh_shading = true,
                .mesh_shading_nv = true,
                .min_lod = true,
                .multiview = true,
-- 
GitLab

