From a38864a874f7fa957a0157251bc8ae2fcd03ef3f Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Mon, 2 May 2022 18:33:15 +0900
Subject: [PATCH 1/2] radv: Add helper for creating an internal queue

Introduce a helper that handles mapping between Vulkan and driver queue
family indices and provides defaults for other parameters like priorities.

Also fix a bug where vk_queue_to_radv could be written out-of-bound if more
than one VkDevice is created from the same VkPhysicalDevice.

Fixes: 177805cc03e ("radv: try and fix internal transfer queue mapping")
---
 src/amd/vulkan/radv_device.c  | 66 ++++++++++++++++++++++++++++++-----
 src/amd/vulkan/radv_private.h | 19 +++++++---
 src/amd/vulkan/radv_wsi.c     | 14 +-------
 3 files changed, 73 insertions(+), 26 deletions(-)

diff --git a/src/amd/vulkan/radv_device.c b/src/amd/vulkan/radv_device.c
index 34a8bba29421..eee1e204ddd9 100644
--- a/src/amd/vulkan/radv_device.c
+++ b/src/amd/vulkan/radv_device.c
@@ -577,18 +577,20 @@ static void
 radv_physical_device_init_queue_table(struct radv_physical_device *pdevice)
 {
    int idx = 0;
-   pdevice->vk_queue_to_radv[idx] = RADV_QUEUE_GENERAL;
-   idx++;
-
-   for (unsigned i = 1; i < RADV_MAX_QUEUE_FAMILIES; i++)
-      pdevice->vk_queue_to_radv[i] = RADV_MAX_QUEUE_FAMILIES + 1;
+   int hidden_idx = RADV_MAX_QUEUE_FAMILIES - 1;
+   pdevice->vk_queue_to_radv[idx++] = RADV_QUEUE_GENERAL;
 
    if (pdevice->rad_info.num_rings[RING_COMPUTE] > 0 &&
        !(pdevice->instance->debug_flags & RADV_DEBUG_NO_COMPUTE_QUEUE)) {
-      pdevice->vk_queue_to_radv[idx] = RADV_QUEUE_COMPUTE;
-      idx++;
+      pdevice->vk_queue_to_radv[idx++] = RADV_QUEUE_COMPUTE;
+   } else {
+      pdevice->vk_queue_to_radv[hidden_idx--] = RADV_QUEUE_COMPUTE;
    }
-   pdevice->num_queues = idx;
+   pdevice->vk_queue_to_radv[hidden_idx--] = RADV_QUEUE_TRANSFER;
+   assert(idx == hidden_idx + 1);
+
+   for (unsigned i = 0; i < RADV_MAX_QUEUE_FAMILIES; i++)
+      pdevice->radv_queue_to_vk[pdevice->vk_queue_to_radv[i]] = i;
 }
 
 static VkResult
@@ -2683,7 +2685,53 @@ radv_get_queue_global_priority(const VkDeviceQueueGlobalPriorityCreateInfoEXT *p
    }
 }
 
-int
+static int
+radv_get_next_internal_queue_idx(struct radv_device *device, enum radv_queue_family qf)
+{
+   uint32_t queue_idx = device->internal_queue_count[qf]++;
+   uint32_t num_rings = device->physical_device->rad_info
+                           .num_rings[radv_queue_family_to_ring(device->physical_device, qf)];
+   assert(num_rings != 0);
+   /* Start from rings with the highest index to reduce contention with application queues. */
+   return num_rings - 1 - queue_idx % num_rings;
+}
+
+VkResult
+radv_internal_queue_init(struct radv_device *device, struct radv_queue *queue,
+                         enum radv_queue_family qf)
+{
+   float prio = 0.5f;
+
+   const VkDeviceQueueCreateInfo queue_create = {
+      .sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO,
+      .queueFamilyIndex = device->physical_device->radv_queue_to_vk[qf],
+      .queueCount = 1,
+      .pQueuePriorities = &prio,
+   };
+
+   /*
+    * A hw_ctx is only created for the priorities the application has requested, so
+    * global_priority == NULL might not always work. Here, we try to pick a valid priority value.
+    * For now, assume that driver internal work should have the highest priority.
+    * If there are priority inversion issues, we can revisit the selection logic.
+    */
+   VkDeviceQueueGlobalPriorityCreateInfoEXT queue_prio_crinfo = {
+      .globalPriority = VK_QUEUE_GLOBAL_PRIORITY_REALTIME_EXT,
+   };
+   while (!device->hw_ctx[radv_get_queue_global_priority(&queue_prio_crinfo)]) {
+      queue_prio_crinfo.globalPriority >>= 1;
+   }
+
+   VkResult result = radv_queue_init(device, queue, radv_get_next_internal_queue_idx(device, qf),
+                                     &queue_create, &queue_prio_crinfo);
+   if (result == VK_SUCCESS) {
+      /* Hide this queue from the application. The freeing should be handled in radv_DestroyDevice. */
+      list_delinit(&queue->vk.link);
+   }
+   return result;
+}
+
+VkResult
 radv_queue_init(struct radv_device *device, struct radv_queue *queue, int idx,
                 const VkDeviceQueueCreateInfo *create_info,
                 const VkDeviceQueueGlobalPriorityCreateInfoEXT *global_priority)
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index fb96feb40924..4125f71e6d70 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -325,8 +325,15 @@ struct radv_physical_device {
 
    nir_shader_compiler_options nir_options[MESA_VULKAN_SHADER_STAGES];
 
+   /* Queue mapping between Vulkan queue family indices and RADV queue family constants.
+    * These two do not necessarily match because some queue families are not exposed to
+    * applications (e.g. TRANSFER/SDMA queues).
+    * We assign public queue family indices from the beginning (so that those are contiguous),
+    * while putting internal queue family indices from the end so that we can still create those
+    * queues for driver use.
+    */
    enum radv_queue_family vk_queue_to_radv[RADV_MAX_QUEUE_FAMILIES];
-   uint32_t num_queues;
+   uint32_t radv_queue_to_vk[RADV_MAX_QUEUE_FAMILIES];
 };
 
 struct radv_instance {
@@ -772,6 +779,7 @@ struct radv_device {
 
    struct radv_queue *queues[RADV_MAX_QUEUE_FAMILIES];
    int queue_count[RADV_MAX_QUEUE_FAMILIES];
+   int internal_queue_count[RADV_MAX_QUEUE_FAMILIES];
 
    bool pbb_allowed;
    uint32_t tess_offchip_block_dw_size;
@@ -2641,9 +2649,12 @@ struct radv_query_pool {
 
 bool radv_queue_internal_submit(struct radv_queue *queue, struct radeon_cmdbuf *cs);
 
-int radv_queue_init(struct radv_device *device, struct radv_queue *queue, int idx,
-                    const VkDeviceQueueCreateInfo *create_info,
-                    const VkDeviceQueueGlobalPriorityCreateInfoEXT *global_priority);
+VkResult radv_internal_queue_init(struct radv_device *device, struct radv_queue *queue,
+                                  enum radv_queue_family qf);
+
+VkResult radv_queue_init(struct radv_device *device, struct radv_queue *queue, int idx,
+                         const VkDeviceQueueCreateInfo *create_info,
+                         const VkDeviceQueueGlobalPriorityCreateInfoEXT *global_priority);
 
 void radv_set_descriptor_set(struct radv_cmd_buffer *cmd_buffer, VkPipelineBindPoint bind_point,
                              struct radv_descriptor_set *set, unsigned idx);
diff --git a/src/amd/vulkan/radv_wsi.c b/src/amd/vulkan/radv_wsi.c
index 72b21659502c..51088617778a 100644
--- a/src/amd/vulkan/radv_wsi.c
+++ b/src/amd/vulkan/radv_wsi.c
@@ -60,23 +60,11 @@ radv_wsi_get_prime_blit_queue(VkDevice _device)
 
    if (device->physical_device->rad_info.chip_class >= GFX9 &&
        !(device->physical_device->instance->debug_flags & RADV_DEBUG_NO_DMA_BLIT)) {
-
-      device->physical_device->vk_queue_to_radv[device->physical_device->num_queues++] = RADV_QUEUE_TRANSFER;
-      const VkDeviceQueueCreateInfo queue_create = {
-         .sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO,
-         .queueFamilyIndex = device->physical_device->num_queues - 1,
-         .queueCount = 1,
-      };
-
       device->private_sdma_queue = vk_zalloc(&device->vk.alloc, sizeof(struct radv_queue), 8,
                                              VK_SYSTEM_ALLOCATION_SCOPE_DEVICE);
 
-      VkResult result = radv_queue_init(device, device->private_sdma_queue, 0, &queue_create, NULL);
+      VkResult result = radv_internal_queue_init(device, device->private_sdma_queue, RADV_QUEUE_TRANSFER);
       if (result == VK_SUCCESS) {
-         /* Remove the queue from our queue list because it'll be cleared manually
-          * in radv_DestroyDevice.
-          */
-         list_delinit(&device->private_sdma_queue->vk.link);
          return vk_queue_to_handle(&device->private_sdma_queue->vk);
       } else {
          vk_free(&device->vk.alloc, device->private_sdma_queue);
-- 
GitLab


From c4204adac32ecab891c8d5ae1951d26c4a05d63d Mon Sep 17 00:00:00 2001
From: Tatsuyuki Ishi <ishitatsuyuki@gmail.com>
Date: Sun, 1 May 2022 17:20:40 +0900
Subject: [PATCH 2/2] radv: Upload shaders to invisible VRAM for non-resizable
 BAR systems

Following PAL's implementation, this patch avoids allocating shader code
buffers in BAR and use compute DMA to upload them to invisible VRAM
directly.

For some games like HZD, shaders can take as much as 400MB, which exceeds
the non-resizable BAR size (256MB) and cause inconsistent spilling
behavior. The kernel will normally move these to invisible VRAM on its own,
but there are potential cases that leads to bad performance:
1. We try to upload a new shader to an arena in invisible VRAM, which
causes the BO to be paged back into GTT, slowing down every draw call using
   that shader.
2. The application allocates a large descriptor set first then creates the
   pipelines. The large pipeline footprint will cause the descriptors to be
   evicted to GTT. (e.g. vkd3d-proton)

By not using the BAR heap at all, we avoid overcommiting and all of these
potential edge cases from happening.

Tested-by: Mike Lothian <mike@fireburn.co.uk>
---
 src/amd/vulkan/radv_cmd_buffer.c |   8 +
 src/amd/vulkan/radv_constants.h  |   2 +
 src/amd/vulkan/radv_device.c     |  70 ++++++++-
 src/amd/vulkan/radv_pipeline.c   |  62 ++++++--
 src/amd/vulkan/radv_private.h    |  21 +++
 src/amd/vulkan/radv_shader.c     | 248 ++++++++++++++++++++++++++++---
 src/amd/vulkan/radv_shader.h     |  12 +-
 7 files changed, 386 insertions(+), 37 deletions(-)

diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index d2cb269a2501..4e2fe04a3dd5 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -517,6 +517,7 @@ radv_reset_cmd_buffer(struct radv_cmd_buffer *cmd_buffer)
    cmd_buffer->gds_needed = false;
    cmd_buffer->gds_oa_needed = false;
    cmd_buffer->sample_positions_needed = false;
+   cmd_buffer->needed_shader_upload_sem = 0;
 
    if (cmd_buffer->upload.upload_bo)
       radv_cs_add_buffer(cmd_buffer->device->ws, cmd_buffer->cs, cmd_buffer->upload.upload_bo);
@@ -3036,6 +3037,7 @@ radv_emit_vertex_input(struct radv_cmd_buffer *cmd_buffer, bool pipeline_is_dirt
       cmd_buffer->record_result = VK_ERROR_OUT_OF_HOST_MEMORY;
       return;
    }
+   cmd_buffer->needed_shader_upload_sem = MAX2(cmd_buffer->needed_shader_upload_sem, prolog->upload_sem);
    emit_prolog_regs(cmd_buffer, vs_shader, prolog, pipeline_is_dirty);
    emit_prolog_inputs(cmd_buffer, vs_shader, nontrivial_divisors, pipeline_is_dirty);
 
@@ -5107,6 +5109,10 @@ radv_CmdBindPipeline(VkCommandBuffer commandBuffer, VkPipelineBindPoint pipeline
    RADV_FROM_HANDLE(radv_cmd_buffer, cmd_buffer, commandBuffer);
    RADV_FROM_HANDLE(radv_pipeline, pipeline, _pipeline);
 
+   if (pipeline)
+      cmd_buffer->needed_shader_upload_sem =
+         MAX2(cmd_buffer->needed_shader_upload_sem, pipeline->upload_sem);
+
    switch (pipelineBindPoint) {
    case VK_PIPELINE_BIND_POINT_COMPUTE:
       if (cmd_buffer->state.compute_pipeline == pipeline)
@@ -5720,6 +5726,8 @@ radv_CmdExecuteCommands(VkCommandBuffer commandBuffer, uint32_t commandBufferCou
       if (secondary->gds_needed)
          primary->gds_needed = true;
 
+      primary->needed_shader_upload_sem = MAX2(primary->needed_shader_upload_sem, secondary->needed_shader_upload_sem);
+
       if (!secondary->state.framebuffer && primary->state.pass && (primary->state.dirty & RADV_CMD_DIRTY_FRAMEBUFFER)) {
          /* Emit the framebuffer state from primary if secondary
           * has been recorded without a framebuffer, otherwise
diff --git a/src/amd/vulkan/radv_constants.h b/src/amd/vulkan/radv_constants.h
index 5787f0423161..5c82db3e9ae0 100644
--- a/src/amd/vulkan/radv_constants.h
+++ b/src/amd/vulkan/radv_constants.h
@@ -106,4 +106,6 @@
 #define RADV_SHADER_ALLOC_NUM_FREE_LISTS                                                           \
    (RADV_SHADER_ALLOC_MAX_SIZE_CLASS - RADV_SHADER_ALLOC_MIN_SIZE_CLASS + 1)
 
+#define RADV_SHADER_UPLOAD_CS_COUNT 16
+
 #endif /* RADV_CONSTANTS_H */
diff --git a/src/amd/vulkan/radv_device.c b/src/amd/vulkan/radv_device.c
index eee1e204ddd9..0f05966a1670 100644
--- a/src/amd/vulkan/radv_device.c
+++ b/src/amd/vulkan/radv_device.c
@@ -28,6 +28,7 @@
 #include <fcntl.h>
 #include <stdbool.h>
 #include <string.h>
+#include "vk_common_entrypoints.h"
 
 #ifdef __FreeBSD__
 #include <sys/types.h>
@@ -2705,7 +2706,12 @@ radv_internal_queue_init(struct radv_device *device, struct radv_queue *queue,
    const VkDeviceQueueCreateInfo queue_create = {
       .sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO,
       .queueFamilyIndex = device->physical_device->radv_queue_to_vk[qf],
-      .queueCount = 1,
+      /*
+       * HACK: Pretend that we are creating a queue for each ring, so that this will not trip the
+       *       assertion in vk_queue_init().
+       */
+      .queueCount = device->physical_device->rad_info
+                       .num_rings[radv_queue_family_to_ring(device->physical_device, qf)],
       .pQueuePriorities = &prio,
    };
 
@@ -2750,7 +2756,7 @@ radv_queue_init(struct radv_device *device, struct radv_queue *queue, int idx,
    return VK_SUCCESS;
 }
 
-static void
+void
 radv_queue_finish(struct radv_queue *queue)
 {
    if (queue->initial_full_flush_preamble_cs)
@@ -3303,8 +3309,6 @@ radv_CreateDevice(VkPhysicalDevice physicalDevice, const VkDeviceCreateInfo *pCr
 
    device->image_float32_atomics = image_float32_atomics;
 
-   radv_init_shader_arenas(device);
-
    device->overallocation_disallowed = overallocation_disallowed;
    mtx_init(&device->overallocation_mutex, mtx_plain);
 
@@ -3349,6 +3353,11 @@ radv_CreateDevice(VkPhysicalDevice physicalDevice, const VkDeviceCreateInfo *pCr
    }
    device->private_sdma_queue = VK_NULL_HANDLE;
 
+   device->shader_use_invisible_vram = radv_shader_use_invisible_vram(&device->physical_device->rad_info, device->instance->perftest_flags);
+   device->keep_shader_staging_buf = radv_thread_trace_enabled();
+   if (radv_init_shader_arenas(device) != VK_SUCCESS)
+      goto fail;
+
    device->pbb_allowed = device->physical_device->rad_info.chip_class >= GFX9 &&
                          !(device->instance->debug_flags & RADV_DEBUG_NOBINNING);
 
@@ -4566,6 +4575,59 @@ struct radv_deferred_queue_submission {
    struct list_head processing_list;
 };
 
+VKAPI_ATTR VkResult VKAPI_CALL
+radv_QueueSubmit2KHR(VkQueue _queue, uint32_t submitCount, const VkSubmitInfo2KHR *pSubmits,
+                     VkFence _fence)
+{
+   RADV_FROM_HANDLE(radv_queue, queue, _queue);
+
+   for (uint32_t i = 0; i < submitCount; i++) {
+      uint64_t needed_shader_upload_sem = 0;
+      for (uint32_t j = 0; j < pSubmits->commandBufferInfoCount; j++) {
+         RADV_FROM_HANDLE(radv_cmd_buffer, cmd_buffer,
+                          pSubmits->pCommandBufferInfos[j].commandBuffer);
+         needed_shader_upload_sem =
+            MAX2(needed_shader_upload_sem, cmd_buffer->needed_shader_upload_sem);
+      }
+      VkResult result;
+      if (needed_shader_upload_sem > queue->last_shader_upload_sem) {
+         uint32_t wait_sem_count = pSubmits[i].waitSemaphoreInfoCount;
+         VkSemaphoreSubmitInfoKHR *new_wait_sems =
+            alloca(sizeof(VkSemaphoreSubmitInfoKHR) * (wait_sem_count + 1));
+         memcpy(new_wait_sems, pSubmits[i].pWaitSemaphoreInfos,
+                sizeof(VkSemaphoreSubmitInfoKHR) * wait_sem_count);
+         new_wait_sems[wait_sem_count] = (VkSemaphoreSubmitInfoKHR){
+            .sType = VK_STRUCTURE_TYPE_SEMAPHORE_SUBMIT_INFO_KHR,
+            .semaphore = queue->device->shader_upload_sem,
+            .deviceIndex = 0,
+            .stageMask = VK_PIPELINE_STAGE_2_ALL_COMMANDS_BIT,
+            .value = needed_shader_upload_sem,
+         };
+
+         const VkSubmitInfo2KHR info = {
+            .sType = VK_STRUCTURE_TYPE_SUBMIT_INFO_2_KHR,
+            .pNext = pSubmits[i].pNext,
+            .commandBufferInfoCount = pSubmits[i].commandBufferInfoCount,
+            .pCommandBufferInfos = pSubmits[i].pCommandBufferInfos,
+            .waitSemaphoreInfoCount = wait_sem_count + 1,
+            .pWaitSemaphoreInfos = new_wait_sems,
+            .signalSemaphoreInfoCount = pSubmits[i].signalSemaphoreInfoCount,
+            .pSignalSemaphoreInfos = pSubmits[i].pSignalSemaphoreInfos,
+         };
+         result = vk_common_QueueSubmit2KHR(_queue, submitCount, &info,
+                                            i == submitCount - 1 ? _fence : VK_NULL_HANDLE);
+      } else {
+         result = vk_common_QueueSubmit2KHR(_queue, submitCount, &pSubmits[i],
+                                            i == submitCount - 1 ? _fence : VK_NULL_HANDLE);
+      }
+      if (unlikely(result != VK_SUCCESS))
+         return result;
+      queue->last_shader_upload_sem = MAX2(queue->last_shader_upload_sem, needed_shader_upload_sem);
+   }
+
+   return VK_SUCCESS;
+}
+
 static VkResult
 radv_queue_submit(struct vk_queue *vqueue, struct vk_queue_submit *submission)
 {
diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index 1b84ccbb77d7..8b7d0f404239 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -248,6 +248,14 @@ radv_pipeline_destroy(struct radv_device *device, struct radv_pipeline *pipeline
       free(pipeline->library.stages);
    }
 
+   const VkSemaphoreWaitInfo wait_info = {
+      .sType = VK_STRUCTURE_TYPE_SEMAPHORE_WAIT_INFO,
+      .pSemaphores = &device->shader_upload_sem,
+      .semaphoreCount = 1,
+      .pValues = &pipeline->upload_sem,
+   };
+   device->vk.dispatch_table.WaitSemaphores(radv_device_to_handle(device), &wait_info, UINT64_MAX);
+
    if (pipeline->slab)
       radv_pipeline_slab_destroy(device, pipeline->slab);
 
@@ -258,6 +266,9 @@ radv_pipeline_destroy(struct radv_device *device, struct radv_pipeline *pipeline
    if (pipeline->gs_copy_shader)
       radv_shader_destroy(device, pipeline->gs_copy_shader);
 
+   if (pipeline->shader_upload_buf)
+      free(pipeline->shader_upload_buf);
+
    if (pipeline->cs.buf)
       free(pipeline->cs.buf);
 
@@ -3791,6 +3802,8 @@ radv_upload_shaders(struct radv_device *device, struct radv_pipeline *pipeline,
                     struct radv_shader_binary **binaries, struct radv_shader_binary *gs_copy_binary)
 {
    uint32_t code_size = 0;
+   VkResult result = VK_SUCCESS;
+   char *staging = NULL;
 
    /* Compute the total code size. */
    for (int i = 0; i < MESA_VULKAN_SHADER_STAGES; i++) {
@@ -3808,36 +3821,59 @@ radv_upload_shaders(struct radv_device *device, struct radv_pipeline *pipeline,
    /* Allocate memory for all shader binaries. */
    pipeline->slab = radv_pipeline_slab_create(device, pipeline, code_size);
    if (!pipeline->slab)
-      return VK_ERROR_OUT_OF_DEVICE_MEMORY;
+      goto out_of_host_memory;
+
+   staging = calloc(1, code_size);
 
    /* Upload shader binaries. */
-   uint64_t slab_va = radv_buffer_get_va(pipeline->slab->alloc->arena->bo);
-   uint32_t slab_offset = pipeline->slab->alloc->offset;
-   char *slab_ptr = pipeline->slab->alloc->arena->ptr;
+   uint64_t slab_va = radv_buffer_get_va(pipeline->slab->alloc->arena->bo) + pipeline->slab->alloc->offset;
+   uint32_t offset = 0;
+   char *slab_ptr = pipeline->slab->alloc->arena->ptr + pipeline->slab->alloc->offset;
 
    for (int i = 0; i < MESA_VULKAN_SHADER_STAGES; ++i) {
       struct radv_shader *shader = pipeline->shaders[i];
       if (!shader)
          continue;
 
-      shader->va = slab_va + slab_offset;
+      shader->va = slab_va + offset;
+
+      if (!radv_shader_binary_reloc(device, binaries[i], shader, staging + offset))
+         goto out_of_host_memory;
 
-      void *dest_ptr = slab_ptr + slab_offset;
-      if (!radv_shader_binary_upload(device, binaries[i], shader, dest_ptr))
-         return VK_ERROR_OUT_OF_HOST_MEMORY;
+      if (device->keep_shader_staging_buf) {
+         shader->code_ptr = (uint8_t *)staging + offset;
+      }
 
-      slab_offset += align(shader->code_size, RADV_SHADER_ALLOC_ALIGNMENT);
+      offset += align(shader->code_size, RADV_SHADER_ALLOC_ALIGNMENT);
    }
 
    if (pipeline->gs_copy_shader) {
-      pipeline->gs_copy_shader->va = slab_va + slab_offset;
+      struct radv_shader *shader = pipeline->gs_copy_shader;
+      shader->va = slab_va + offset;
 
-      void *dest_ptr = slab_ptr + slab_offset;
-      if (!radv_shader_binary_upload(device, gs_copy_binary, pipeline->gs_copy_shader, dest_ptr))
-         return VK_ERROR_OUT_OF_HOST_MEMORY;
+      if (!radv_shader_binary_reloc(device, gs_copy_binary, shader, staging + offset))
+         goto out_of_host_memory;
+   }
+
+   result = radv_shader_binary_upload(device, pipeline->slab->alloc->arena->bo, slab_va, slab_ptr,
+                                      staging, code_size, &pipeline->upload_sem);
+   if (result != VK_SUCCESS)
+      goto err;
+
+   if (device->keep_shader_staging_buf) {
+      pipeline->shader_upload_buf = staging;
+   } else {
+      free(staging);
    }
 
    return VK_SUCCESS;
+
+out_of_host_memory:
+   result = VK_ERROR_OUT_OF_HOST_MEMORY;
+   /* fallthrough */
+err:
+   free(staging);
+   return result;
 }
 
 static bool
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index 4125f71e6d70..7a9d8f5d6315 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -715,6 +715,8 @@ struct radv_queue {
    struct radeon_winsys_ctx *hw_ctx;
    enum radeon_ctx_priority priority;
 
+   uint64_t last_shader_upload_sem;
+
    enum radv_queue_family qf;
    uint32_t scratch_size_per_wave;
    uint32_t scratch_waves;
@@ -825,6 +827,18 @@ struct radv_device {
    struct list_head shader_block_obj_pool;
    mtx_t shader_arena_mutex;
 
+   mtx_t shader_upload_mutex;
+   struct radv_queue *shader_upload_queue;
+   uint64_t shader_upload_sem_val;
+   VkSemaphore shader_upload_sem;
+   VkCommandPool shader_upload_command_pool;
+   VkCommandBuffer shader_upload_command_bufs[RADV_SHADER_UPLOAD_CS_COUNT];
+
+   /* Whether to DMA shaders to invisible VRAM or to upload directly through BAR. */
+   bool shader_use_invisible_vram;
+   /* If true, retain shaders in system RAM to allow dumping to SQTT traces. */
+   bool keep_shader_staging_buf;
+
    /* For detecting VM faults reported by dmesg. */
    uint64_t dmesg_timestamp;
 
@@ -1569,6 +1583,8 @@ struct radv_cmd_buffer {
     * Bitmask of pending active query flushes.
     */
    enum radv_cmd_flush_bits active_query_flush_bits;
+
+   uint64_t needed_shader_upload_sem;
 };
 
 struct radv_image;
@@ -1877,6 +1893,9 @@ struct radv_pipeline {
    struct radv_shader *gs_copy_shader;
    VkShaderStageFlags active_stages;
 
+   char *shader_upload_buf;
+   uint64_t upload_sem;
+
    struct radeon_cmdbuf cs;
    uint32_t ctx_cs_hash;
    struct radeon_cmdbuf ctx_cs;
@@ -2656,6 +2675,8 @@ VkResult radv_queue_init(struct radv_device *device, struct radv_queue *queue, i
                          const VkDeviceQueueCreateInfo *create_info,
                          const VkDeviceQueueGlobalPriorityCreateInfoEXT *global_priority);
 
+void radv_queue_finish(struct radv_queue *queue);
+
 void radv_set_descriptor_set(struct radv_cmd_buffer *cmd_buffer, VkPipelineBindPoint bind_point,
                              struct radv_descriptor_set *set, unsigned idx);
 
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 60ef16c037fb..f8b122dfb185 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -1243,6 +1243,13 @@ free_block_obj(struct radv_device *device, union radv_shader_arena_block *block)
    list_add(&block->pool, &device->shader_block_obj_pool);
 }
 
+bool
+radv_shader_use_invisible_vram(const struct radeon_info *info, uint32_t perftest)
+{
+   return !(perftest & RADV_PERFTEST_SAM) &&
+          ((perftest & RADV_PERFTEST_NO_SAM) || !info->all_vram_visible);
+}
+
 /* Segregated fit allocator, implementing a good-fit allocation policy.
  *
  * This is an variation of sequential fit allocation with several lists of free blocks ("holes")
@@ -1318,20 +1325,32 @@ radv_alloc_shader_memory(struct radv_device *device, uint32_t size, void *ptr)
       MAX2(RADV_SHADER_ALLOC_MIN_ARENA_SIZE
               << MIN2(RADV_SHADER_ALLOC_MAX_ARENA_SIZE_SHIFT, device->shader_arena_shift),
            size);
-   VkResult result = device->ws->buffer_create(
-      device->ws, arena_size, RADV_SHADER_ALLOC_ALIGNMENT, RADEON_DOMAIN_VRAM,
-      RADEON_FLAG_NO_INTERPROCESS_SHARING | RADEON_FLAG_32BIT |
-         (device->physical_device->rad_info.cpdma_prefetch_writes_memory ? 0
-                                                                         : RADEON_FLAG_READ_ONLY),
-      RADV_BO_PRIORITY_SHADER, 0, &arena->bo);
+   bool use_invisible = device->shader_use_invisible_vram;
+   VkResult result;
+   if (use_invisible) {
+      result = device->ws->buffer_create(
+         device->ws, arena_size, RADV_SHADER_ALLOC_ALIGNMENT, RADEON_DOMAIN_VRAM,
+         RADEON_FLAG_NO_INTERPROCESS_SHARING | RADEON_FLAG_32BIT | RADEON_FLAG_NO_CPU_ACCESS,
+         RADV_BO_PRIORITY_SHADER, 0, &arena->bo);
+   } else {
+      result = device->ws->buffer_create(
+         device->ws, arena_size, RADV_SHADER_ALLOC_ALIGNMENT, RADEON_DOMAIN_VRAM,
+         RADEON_FLAG_NO_INTERPROCESS_SHARING | RADEON_FLAG_32BIT | RADEON_FLAG_CPU_ACCESS |
+            (device->physical_device->rad_info.cpdma_prefetch_writes_memory
+                ? 0
+                : RADEON_FLAG_READ_ONLY),
+         RADV_BO_PRIORITY_SHADER, 0, &arena->bo);
+   }
    if (result != VK_SUCCESS)
       goto fail;
 
    list_inithead(&arena->entries);
 
-   arena->ptr = (char *)device->ws->buffer_map(arena->bo);
-   if (!arena->ptr)
-      goto fail;
+   if (!use_invisible) {
+      arena->ptr = (char *)device->ws->buffer_map(arena->bo);
+      if (!arena->ptr)
+         goto fail;
+   }
 
    alloc = alloc_block_obj(device);
    hole = arena_size - size > 0 ? alloc_block_obj(device) : alloc;
@@ -1426,9 +1445,12 @@ radv_free_shader_memory(struct radv_device *device, union radv_shader_arena_bloc
    mtx_unlock(&device->shader_arena_mutex);
 }
 
-void
+VkResult
 radv_init_shader_arenas(struct radv_device *device)
 {
+   VkResult result = VK_SUCCESS;
+   VkDevice vk_device = radv_device_to_handle(device);
+   const struct vk_device_dispatch_table *disp = &device->vk.dispatch_table;
    mtx_init(&device->shader_arena_mutex, mtx_plain);
 
    device->shader_free_list_mask = 0;
@@ -1437,11 +1459,76 @@ radv_init_shader_arenas(struct radv_device *device)
    list_inithead(&device->shader_block_obj_pool);
    for (unsigned i = 0; i < RADV_SHADER_ALLOC_NUM_FREE_LISTS; i++)
       list_inithead(&device->shader_free_lists[i]);
+
+   mtx_init(&device->shader_upload_mutex, mtx_plain);
+   device->shader_upload_queue =
+      vk_zalloc(&device->vk.alloc, sizeof(struct radv_queue), 8, VK_SYSTEM_ALLOCATION_SCOPE_DEVICE);
+   if (!device->shader_upload_queue)
+      return VK_ERROR_OUT_OF_HOST_MEMORY;
+   result = radv_internal_queue_init(device, device->shader_upload_queue, RADV_QUEUE_COMPUTE);
+   if (result != VK_SUCCESS)
+      goto fail;
+   const VkCommandPoolCreateInfo pool_create = {
+      .sType = VK_STRUCTURE_TYPE_COMMAND_POOL_CREATE_INFO,
+      .flags = VK_COMMAND_POOL_CREATE_RESET_COMMAND_BUFFER_BIT,
+      .queueFamilyIndex = device->physical_device->radv_queue_to_vk[RADV_QUEUE_COMPUTE],
+   };
+   result = disp->CreateCommandPool(vk_device, &pool_create, NULL,
+                          &device->shader_upload_command_pool);
+   if (result != VK_SUCCESS)
+      goto fail;
+   const VkCommandBufferAllocateInfo cmd_buf_alloc = {
+      .sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO,
+      .commandPool = device->shader_upload_command_pool,
+      .level = VK_COMMAND_BUFFER_LEVEL_PRIMARY,
+      .commandBufferCount = RADV_SHADER_UPLOAD_CS_COUNT,
+   };
+   result = disp->AllocateCommandBuffers(vk_device, &cmd_buf_alloc,
+                               device->shader_upload_command_bufs);
+   if (result != VK_SUCCESS)
+      goto fail;
+   const VkSemaphoreTypeCreateInfo sem_type = {
+      .sType = VK_STRUCTURE_TYPE_SEMAPHORE_TYPE_CREATE_INFO,
+      .semaphoreType = VK_SEMAPHORE_TYPE_TIMELINE,
+      .initialValue = 0,
+   };
+   const VkSemaphoreCreateInfo sem_create = {
+      .sType = VK_STRUCTURE_TYPE_SEMAPHORE_CREATE_INFO,
+      .pNext = &sem_type,
+   };
+   result = disp->CreateSemaphore(vk_device, &sem_create, NULL,
+                             &device->shader_upload_sem);
+   if (result != VK_SUCCESS)
+      goto fail;
+   return VK_SUCCESS;
+
+fail:
+   if (device->shader_upload_sem)
+      disp->DestroySemaphore(vk_device, device->shader_upload_sem, NULL);
+   if (device->shader_upload_command_bufs[0])
+      disp->FreeCommandBuffers(vk_device, device->shader_upload_command_pool,
+                               RADV_SHADER_UPLOAD_CS_COUNT, device->shader_upload_command_bufs);
+   if (device->shader_upload_command_pool)
+      disp->DestroyCommandPool(vk_device, device->shader_upload_command_pool, NULL);
+
+   radv_queue_finish(device->shader_upload_queue);
+   vk_free(&device->vk.alloc, device->shader_upload_queue);
+   return result;
 }
 
 void
 radv_destroy_shader_arenas(struct radv_device *device)
 {
+   struct vk_device_dispatch_table *disp = &device->vk.dispatch_table;
+
+   /* Upload queue should be idle assuming that pipelines are not leaked */
+   disp->DestroySemaphore(radv_device_to_handle(device), device->shader_upload_sem, NULL);
+   disp->FreeCommandBuffers(radv_device_to_handle(device), device->shader_upload_command_pool,
+                           RADV_SHADER_UPLOAD_CS_COUNT, device->shader_upload_command_bufs);
+   disp->DestroyCommandPool(radv_device_to_handle(device), device->shader_upload_command_pool, NULL);
+   radv_queue_finish(device->shader_upload_queue);
+   vk_free(&device->vk.alloc, device->shader_upload_queue);
+
    list_for_each_entry_safe(union radv_shader_arena_block, block, &device->shader_block_obj_pool,
                             pool) free(block);
 
@@ -1786,7 +1873,7 @@ radv_open_rtld_binary(struct radv_device *device, const struct radv_shader *shad
 }
 
 bool
-radv_shader_binary_upload(struct radv_device *device, const struct radv_shader_binary *binary,
+radv_shader_binary_reloc(struct radv_device *device, const struct radv_shader_binary *binary,
                           struct radv_shader *shader, void *dest_ptr)
 {
    if (binary->type == RADV_BINARY_TYPE_RTLD) {
@@ -1809,7 +1896,6 @@ radv_shader_binary_upload(struct radv_device *device, const struct radv_shader_b
          return false;
       }
 
-      shader->code_ptr = dest_ptr;
       ac_rtld_close(&rtld_binary);
    } else {
       struct radv_shader_binary_legacy *bin = (struct radv_shader_binary_legacy *)binary;
@@ -1819,13 +1905,113 @@ radv_shader_binary_upload(struct radv_device *device, const struct radv_shader_b
       uint32_t *ptr32 = (uint32_t *)dest_ptr + bin->code_size / 4;
       for (unsigned i = 0; i < DEBUGGER_NUM_MARKERS; i++)
          ptr32[i] = DEBUGGER_END_OF_CODE_MARKER;
-
-      shader->code_ptr = dest_ptr;
    }
 
    return true;
 }
 
+/**
+ * When using invisible VRAM for shaders, schedule the upload using DMA. Otherwise, memcpy staging
+ * to mapped.
+ * If out_sem_val is NULL, this function blocks until the DMA is complete. Otherwise, the semaphore
+ * value to wait on device->shader_upload_sem is stored in *out_sem_val.
+ */
+VkResult
+radv_shader_binary_upload(struct radv_device *device, struct radeon_winsys_bo *bo, uint64_t va,
+                          char *mapped, char *staging, unsigned len, uint64_t *out_sem_val)
+{
+   if (!device->shader_use_invisible_vram) {
+      memcpy(mapped, staging, len);
+      return VK_SUCCESS;
+   }
+
+   VkResult result = VK_SUCCESS;
+
+   mtx_lock(&device->shader_upload_mutex);
+   uint64_t sem_val = ++device->shader_upload_sem_val;
+   uint64_t i = sem_val % RADV_SHADER_UPLOAD_CS_COUNT;
+   struct vk_device_dispatch_table *disp = &device->vk.dispatch_table;
+   VkCommandBuffer cmd_buf = device->shader_upload_command_bufs[i];
+   RADV_FROM_HANDLE(radv_cmd_buffer, cmd_buffer, cmd_buf);
+
+   /* Make sure we don't reset an in-flight command buffer */
+   uint64_t wait_val = MAX2(RADV_SHADER_UPLOAD_CS_COUNT, sem_val) - RADV_SHADER_UPLOAD_CS_COUNT;
+   const VkSemaphoreWaitInfo wait_info = {
+      .sType = VK_STRUCTURE_TYPE_SEMAPHORE_WAIT_INFO,
+      .pSemaphores = &device->shader_upload_sem,
+      .semaphoreCount = 1,
+      .pValues = &wait_val,
+   };
+   result = disp->WaitSemaphores(radv_device_to_handle(device), &wait_info, UINT64_MAX);
+   if (unlikely(result != VK_SUCCESS))
+      goto out;
+   result = disp->ResetCommandBuffer(cmd_buf, 0);
+   if (unlikely(result != VK_SUCCESS))
+      goto out;
+
+   const VkCommandBufferBeginInfo begin_info = {
+      .sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO,
+   };
+   result = disp->BeginCommandBuffer(cmd_buf, &begin_info);
+   if (unlikely(result != VK_SUCCESS))
+      goto out;
+
+   uint32_t upload_offset;
+   if (!radv_cmd_buffer_upload_data(cmd_buffer, len, staging, &upload_offset)) {
+      result = VK_ERROR_OUT_OF_DEVICE_MEMORY;
+      goto out;
+   }
+   radv_cs_add_buffer(device->ws, cmd_buffer->cs, bo);
+   /* Compute-based copy requires shaders which might haven't been uploaded yet, so force CP DMA
+    * even if it might be slightly slower */
+   si_cp_dma_buffer_copy(cmd_buffer,
+                         radv_buffer_get_va(cmd_buffer->upload.upload_bo) + upload_offset, va, len);
+
+   result = disp->EndCommandBuffer(cmd_buf);
+   if (unlikely(result != VK_SUCCESS))
+      goto out;
+
+   const VkSemaphoreSubmitInfo signal_info = {
+      .sType = VK_STRUCTURE_TYPE_SEMAPHORE_SUBMIT_INFO,
+      .semaphore = device->shader_upload_sem,
+      .value = sem_val,
+      .stageMask = VK_PIPELINE_STAGE_2_ALL_COMMANDS_BIT,
+   };
+   const VkCommandBufferSubmitInfo buf_info = {
+      .sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_SUBMIT_INFO,
+      .commandBuffer = cmd_buf,
+   };
+   const VkSubmitInfo2 submit_info = {
+      .sType = VK_STRUCTURE_TYPE_SUBMIT_INFO_2,
+      .commandBufferInfoCount = 1,
+      .pCommandBufferInfos = &buf_info,
+      .signalSemaphoreInfoCount = 1,
+      .pSignalSemaphoreInfos = &signal_info,
+   };
+   result = disp->QueueSubmit2KHR(radv_queue_to_handle(device->shader_upload_queue), 1,
+                                  &submit_info, VK_NULL_HANDLE);
+   if (unlikely(result != VK_SUCCESS))
+      goto out;
+
+   if (out_sem_val) {
+      *out_sem_val = sem_val;
+   } else {
+      const VkSemaphoreWaitInfo sync_wait_info = {
+         .sType = VK_STRUCTURE_TYPE_SEMAPHORE_WAIT_INFO,
+         .pSemaphores = &device->shader_upload_sem,
+         .semaphoreCount = 1,
+         .pValues = &sem_val,
+      };
+      result = disp->WaitSemaphores(radv_device_to_handle(device), &sync_wait_info, UINT64_MAX);
+      if (unlikely(result != VK_SUCCESS))
+         goto out;
+   }
+
+out:
+   mtx_unlock(&device->shader_upload_mutex);
+   return result;
+}
+
 struct radv_shader *
 radv_shader_create(struct radv_device *device, const struct radv_shader_binary *binary,
                    bool keep_shader_info, bool from_cache, const struct radv_shader_args *args)
@@ -2085,10 +2271,18 @@ radv_create_trap_handler_shader(struct radv_device *device)
    trap->alloc = radv_alloc_shader_memory(device, shader->code_size, NULL);
 
    trap->bo = trap->alloc->arena->bo;
+   uint64_t dest_va = radv_buffer_get_va(trap->alloc->arena->bo) + trap->alloc->offset;
    char *dest_ptr = trap->alloc->arena->ptr + trap->alloc->offset;
 
    struct radv_shader_binary_legacy *bin = (struct radv_shader_binary_legacy *)binary;
-   memcpy(dest_ptr, bin->data, bin->code_size);
+   if (radv_shader_binary_upload(device, trap->bo, dest_va, dest_ptr, (char *)bin->data,
+                                 bin->code_size, NULL) != VK_SUCCESS) {
+      radv_free_shader_memory(device, trap->alloc);
+      ralloc_free(b.shader);
+      free(shader);
+      free(binary);
+      return NULL;
+   }
 
    ralloc_free(b.shader);
    free(shader);
@@ -2129,10 +2323,11 @@ upload_vs_prolog(struct radv_device *device, struct radv_prolog_binary *bin, uns
    prolog->bo = prolog->alloc->arena->bo;
    char *dest_ptr = prolog->alloc->arena->ptr + prolog->alloc->offset;
 
-   memcpy(dest_ptr, bin->data, bin->code_size);
+   char *staging = calloc(1, code_size);
+   memcpy(staging, bin->data, bin->code_size);
 
    /* Add end-of-code markers for the UMR disassembler. */
-   uint32_t *ptr32 = (uint32_t *)dest_ptr + bin->code_size / 4;
+   uint32_t *ptr32 = (uint32_t *)staging + bin->code_size / 4;
    for (unsigned i = 0; i < DEBUGGER_NUM_MARKERS; i++)
       ptr32[i] = DEBUGGER_END_OF_CODE_MARKER;
 
@@ -2141,6 +2336,17 @@ upload_vs_prolog(struct radv_device *device, struct radv_prolog_binary *bin, uns
    prolog->num_preserved_sgprs = bin->num_preserved_sgprs;
    prolog->disasm_string = NULL;
 
+   uint64_t slab_va = radv_buffer_get_va(prolog->alloc->arena->bo);
+   uint32_t slab_offset = prolog->alloc->offset;
+
+   if (radv_shader_binary_upload(device, prolog->bo, slab_va + slab_offset, dest_ptr, staging,
+                                 bin->code_size, &prolog->upload_sem) != VK_SUCCESS) {
+      free(staging);
+      free(prolog);
+      return NULL;
+   }
+   free(staging);
+
    return prolog;
 }
 
@@ -2220,6 +2426,14 @@ radv_prolog_destroy(struct radv_device *device, struct radv_shader_prolog *prolo
    if (!prolog)
       return;
 
+   const VkSemaphoreWaitInfo wait_info = {
+      .sType = VK_STRUCTURE_TYPE_SEMAPHORE_WAIT_INFO,
+      .pSemaphores = &device->shader_upload_sem,
+      .semaphoreCount = 1,
+      .pValues = &prolog->upload_sem,
+   };
+   device->vk.dispatch_table.WaitSemaphores(radv_device_to_handle(device), &wait_info, UINT64_MAX);
+
    radv_free_shader_memory(device, prolog->alloc);
    free(prolog->disasm_string);
    free(prolog);
diff --git a/src/amd/vulkan/radv_shader.h b/src/amd/vulkan/radv_shader.h
index 30a2538aa494..7783722b6189 100644
--- a/src/amd/vulkan/radv_shader.h
+++ b/src/amd/vulkan/radv_shader.h
@@ -495,6 +495,7 @@ struct radv_shader_prolog {
    uint32_t rsrc1;
    uint8_t num_preserved_sgprs;
    bool nontrivial_divisors;
+   uint64_t upload_sem;
 
    /* debug only */
    char *disasm_string;
@@ -519,7 +520,7 @@ nir_shader *radv_shader_compile_to_nir(struct radv_device *device,
                                        const struct radv_pipeline_stage *stage,
                                        const struct radv_pipeline_key *key);
 
-void radv_init_shader_arenas(struct radv_device *device);
+VkResult radv_init_shader_arenas(struct radv_device *device);
 void radv_destroy_shader_arenas(struct radv_device *device);
 
 VkResult radv_create_shaders(struct radv_pipeline *pipeline,
@@ -542,8 +543,13 @@ struct radv_shader *radv_shader_compile(
    int shader_count, const struct radv_pipeline_key *key, bool keep_shader_info, bool keep_statistic_info,
    struct radv_shader_binary **binary_out);
 
-bool radv_shader_binary_upload(struct radv_device *device, const struct radv_shader_binary *binary,
-                               struct radv_shader *shader, void *dest_ptr);
+bool radv_shader_binary_reloc(struct radv_device *device, const struct radv_shader_binary *binary,
+                              struct radv_shader *shader, void *dest_ptr);
+VkResult radv_shader_binary_upload(struct radv_device *device, struct radeon_winsys_bo *bo,
+                                   uint64_t va, char *mapped, char *staging, unsigned len,
+                                   uint64_t *out_sem_val);
+
+bool radv_shader_use_invisible_vram(const struct radeon_info *info, uint32_t perftest);
 
 union radv_shader_arena_block *radv_alloc_shader_memory(struct radv_device *device, uint32_t size,
                                                         void *ptr);
-- 
GitLab

